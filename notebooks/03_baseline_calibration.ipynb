{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e35e941",
   "metadata": {},
   "source": [
    "# Baseline Calibration — Full Pipeline Traceability\n",
    "\n",
    "This notebook runs single-agent baselines on a stratified CUAD sample\n",
    "with **full pipeline traceability**: every step from input to evaluation is recorded.\n",
    "\n",
    "**What gets recorded per sample:**\n",
    "- **Input**: system prompt, user message (contract + question injected)\n",
    "- **Raw output**: the complete model response before any parsing\n",
    "- **Parsed output**: extracted clauses after baseline-specific parsing\n",
    "- **Ground truth**: the labeled answer spans from CUAD\n",
    "- **Evaluation**: TP/FP/FN/TN classification, Jaccard similarity, grounding rate\n",
    "- **Usage**: token counts, latency, estimated cost\n",
    "\n",
    "**Pipeline flow:**\n",
    "```\n",
    "Config → Load CUAD → Sample → Build prompt → Call model → Parse response → Evaluate → Save JSONL\n",
    "```\n",
    "\n",
    "**Crash-safe**: Each result is appended to a JSONL file immediately.\n",
    "Re-running the cell skips already-completed samples (resume).\n",
    "\n",
    "**Baselines:**\n",
    "- `B1`: Zero-shot (ContractEval exact replication)\n",
    "- `B4`: Chain-of-Thought\n",
    "- `M6`: Combined specialist prompts (single agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d4b2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION — Change these to switch model / baseline / sample size\n",
    "# ============================================================\n",
    "\n",
    "MODEL_KEY = \"claude-sonnet-4\"       # Model key (see src/models/config.py for all options)\n",
    "BASELINE_TYPE = \"B1\"                # B1=zero-shot, B4=chain-of-thought, M6=combined-prompts\n",
    "SAMPLES_PER_TIER = 10               # Samples per tier (common/moderate/rare)\n",
    "INCLUDE_NEGATIVE_SAMPLES = True    # Include samples where ground truth is empty\n",
    "MAX_CONTRACT_CHARS = 100_000       # Skip contracts longer than this\n",
    "TEMPERATURE = 0.0                  # Generation temperature\n",
    "MAX_TOKENS = 4096                  # Max output tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fd4cb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:    Claude Sonnet 4 (claude-sonnet-4-20250514)\n",
      "Provider: anthropic\n",
      "Baseline: B1 (zero_shot)\n",
      "Context:  200,000 tokens\n",
      "API key:  set\n"
     ]
    }
   ],
   "source": [
    "import sys, os, time, json, datetime\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "from src.models.config import get_model_config, ModelProvider\n",
    "\n",
    "config = get_model_config(MODEL_KEY)\n",
    "baseline_labels = {\"B1\": \"zero_shot\", \"B4\": \"cot\", \"M6\": \"combined_prompts\"}\n",
    "baseline_label = baseline_labels[BASELINE_TYPE]\n",
    "\n",
    "print(f\"Model:    {config.name} ({config.model_id})\")\n",
    "print(f\"Provider: {config.provider.value}\")\n",
    "print(f\"Baseline: {BASELINE_TYPE} ({baseline_label})\")\n",
    "print(f\"Context:  {config.context_window:,} tokens\")\n",
    "\n",
    "# Verify provider connectivity\n",
    "if config.provider == ModelProvider.OLLAMA:\n",
    "    import urllib.request\n",
    "    try:\n",
    "        urllib.request.urlopen(f\"{config.base_url or 'http://localhost:11434/v1'}/models\")\n",
    "        print(\"Ollama:   connected\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING:  Ollama not reachable — {e}\")\n",
    "elif config.provider == ModelProvider.ANTHROPIC:\n",
    "    assert os.getenv(\"ANTHROPIC_API_KEY\"), \"ANTHROPIC_API_KEY not set\"\n",
    "    print(\"API key:  set\")\n",
    "elif config.provider == ModelProvider.OPENAI:\n",
    "    assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not set\"\n",
    "    print(\"API key:  set\")\n",
    "elif config.provider == ModelProvider.GOOGLE:\n",
    "    assert os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\"), \"GEMINI_API_KEY not set\"\n",
    "    print(\"API key:  set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad9ac9",
   "metadata": {},
   "source": [
    "## 1. Load and Sample CUAD Data\n",
    "\n",
    "Stratified sampling: a few from each tier (common / moderate / rare),\n",
    "including both positive (has clause) and negative (no clause) samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d67d2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 20,910\n",
      "Contracts:     510\n",
      "\n",
      "common    : 2,574 samples (2278 pos, 296 neg)\n",
      "moderate  : 7,722 samples (2050 pos, 5672 neg)\n",
      "rare      : 7,293 samples (812 pos, 6481 neg)\n"
     ]
    }
   ],
   "source": [
    "from src.data.cuad_loader import CUADDataLoader, CATEGORY_TIERS\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "loader = CUADDataLoader()\n",
    "loader.load()\n",
    "all_samples = list(loader)\n",
    "\n",
    "print(f\"Total samples: {len(all_samples):,}\")\n",
    "print(f\"Contracts:     {len(loader.get_contracts())}\")\n",
    "print()\n",
    "\n",
    "by_tier: dict[str, list] = defaultdict(list)\n",
    "for s in all_samples:\n",
    "    if len(s.contract_text) <= MAX_CONTRACT_CHARS:\n",
    "        by_tier[s.tier].append(s)\n",
    "\n",
    "for tier in [\"common\", \"moderate\", \"rare\"]:\n",
    "    pos = sum(1 for s in by_tier[tier] if s.has_clause)\n",
    "    neg = len(by_tier[tier]) - pos\n",
    "    print(f\"{tier:10s}: {len(by_tier[tier]):,} samples ({pos} pos, {neg} neg)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73f15890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 9 samples:\n",
      "\n",
      "  [common  ] Agreement Date                           (1 spans) | 32,321 chars\n",
      "  [common  ] Expiration Date                          (1 spans) | 6,341 chars\n",
      "  [common  ] Expiration Date                          (no clause) | 14,313 chars\n",
      "  [moderate] Insurance                                (2 spans) | 55,075 chars\n",
      "  [moderate] Anti-Assignment                          (1 spans) | 8,852 chars\n",
      "  [moderate] Change Of Control                        (no clause) | 6,320 chars\n",
      "  [rare    ] Covenant Not To Sue                      (4 spans) | 42,972 chars\n",
      "  [rare    ] Minimum Commitment                       (3 spans) | 15,202 chars\n",
      "  [rare    ] Revenue/Profit Sharing                   (no clause) | 42,473 chars\n"
     ]
    }
   ],
   "source": [
    "selected = []\n",
    "for tier in [\"common\", \"moderate\", \"rare\"]:\n",
    "    tier_samples = by_tier[tier]\n",
    "    positive = [s for s in tier_samples if s.has_clause]\n",
    "    negative = [s for s in tier_samples if not s.has_clause]\n",
    "\n",
    "    n_pos = min(SAMPLES_PER_TIER, len(positive))\n",
    "    selected.extend(random.sample(positive, n_pos))\n",
    "\n",
    "    if INCLUDE_NEGATIVE_SAMPLES and negative:\n",
    "        n_neg = min(max(1, SAMPLES_PER_TIER // 2), len(negative))\n",
    "        selected.extend(random.sample(negative, n_neg))\n",
    "\n",
    "print(f\"Selected {len(selected)} samples:\\n\")\n",
    "for s in selected:\n",
    "    info = f\"{s.num_spans} spans\" if s.has_clause else \"no clause\"\n",
    "    print(f\"  [{s.tier:8s}] {s.category:40s} ({info}) | {len(s.contract_text):,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e6a8d9",
   "metadata": {},
   "source": [
    "## 2. Run Extraction with Full Traceability\n",
    "\n",
    "Each sample goes through:\n",
    "1. **Build prompt** — baseline-specific system prompt + user message (with contract text and question injected)\n",
    "2. **Call model** — raw API call capturing response text + token usage\n",
    "3. **Parse response** — baseline-specific parser extracts clauses from raw response\n",
    "4. **Evaluate** — classify as TP/FP/FN/TN, compute Jaccard, check grounding\n",
    "5. **Save** — append full record to JSONL immediately (crash-safe)\n",
    "\n",
    "The JSONL file enables **resume**: re-running skips already-completed samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82775f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID:       zero_shot_claude-sonnet-4_20260215_233422\n",
      "Intermediate: ../experiments/results/zero_shot_claude-sonnet-4_20260215_233422_intermediate.jsonl\n",
      "\n",
      "[1/9] Agreement Date (common)... -> TP | 1 clause(s) | J=0.120 | 4.4s\n",
      "[2/9] Expiration Date (common)... -> FN | 0 clause(s) | J=0.000 | 1.2s\n",
      "[3/9] Expiration Date (common)... -> TN | 0 clause(s) | J=1.000 | 1.3s\n",
      "[4/9] Insurance (moderate)... -> TP | 1 clause(s) | J=0.613 | 3.0s\n",
      "[5/9] Anti-Assignment (moderate)... -> TP | 1 clause(s) | J=0.895 | 1.7s\n",
      "[6/9] Change Of Control (moderate)... -> TN | 0 clause(s) | J=1.000 | 1.6s\n",
      "[7/9] Covenant Not To Sue (rare)... -> TP | 3 clause(s) | J=0.676 | 4.5s\n",
      "[8/9] Minimum Commitment (rare)... -> TP | 3 clause(s) | J=0.103 | 3.8s\n",
      "[9/9] Revenue/Profit Sharing (rare)... -> TN | 0 clause(s) | J=1.000 | 1.6s\n",
      "\n",
      "Completed: 9 total (0 resumed)\n",
      "Intermediate saved to: ../experiments/results/zero_shot_claude-sonnet-4_20260215_233422_intermediate.jsonl\n",
      "Total wall time: 23.1s\n"
     ]
    }
   ],
   "source": [
    "from src.models import invoke_model as model_invoke\n",
    "from src.models.diagnostics import ModelDiagnostics, TokenUsage\n",
    "from src.evaluation.metrics import span_overlap, compute_jaccard, compute_grounding_rate\n",
    "\n",
    "# Import baseline prompts and parsers\n",
    "from src.baselines.zero_shot import CONTRACTEVAL_PROMPT, ZeroShotBaseline\n",
    "from src.baselines.chain_of_thought import COT_PROMPT, ChainOfThoughtBaseline\n",
    "from src.baselines.combined_prompts import COMBINED_PROMPT, CombinedPromptsBaseline\n",
    "\n",
    "\n",
    "def build_messages(sample, baseline_type):\n",
    "    \"\"\"Build (system_prompt, user_message) for the selected baseline.\n",
    "\n",
    "    This is exactly what each baseline's extract() method does internally,\n",
    "    but exposed here so we can capture the raw input/output.\n",
    "    \"\"\"\n",
    "    if baseline_type == \"B1\":\n",
    "        system_prompt = CONTRACTEVAL_PROMPT\n",
    "        user_msg = f\"Context:\\n{sample.contract_text}\\n\\nQuestion:\\n{sample.question}\"\n",
    "        return system_prompt, user_msg\n",
    "    elif baseline_type == \"B4\":\n",
    "        system_prompt = None\n",
    "        user_msg = COT_PROMPT.format(\n",
    "            contract_text=sample.contract_text,\n",
    "            question=sample.question,\n",
    "        )\n",
    "        return system_prompt, user_msg\n",
    "    elif baseline_type == \"M6\":\n",
    "        system_prompt = None\n",
    "        user_msg = COMBINED_PROMPT.format(\n",
    "            category=sample.category,\n",
    "            contract_text=sample.contract_text,\n",
    "            question=sample.question,\n",
    "        )\n",
    "        return system_prompt, user_msg\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown baseline type: {baseline_type}\")\n",
    "\n",
    "\n",
    "# Create parser instances (just for their parse methods, not for extraction)\n",
    "_parsers = {\n",
    "    \"B1\": ZeroShotBaseline(),\n",
    "    \"B4\": ChainOfThoughtBaseline(),\n",
    "    \"M6\": CombinedPromptsBaseline(),\n",
    "}\n",
    "\n",
    "\n",
    "def parse_response(raw_response, category, baseline_type):\n",
    "    \"\"\"Parse raw model response using the baseline-specific parser.\"\"\"\n",
    "    parser = _parsers[baseline_type]\n",
    "    if baseline_type in (\"B1\", \"B4\"):\n",
    "        result = parser.parse_response(raw_response)\n",
    "        result.category = category\n",
    "        return result\n",
    "    elif baseline_type == \"M6\":\n",
    "        data = parser.parse_json_response(raw_response)\n",
    "        if data and \"extracted_clauses\" in data:\n",
    "            result = parser.result_from_dict(data, category)\n",
    "        else:\n",
    "            result = parser._parse_plaintext(raw_response, category)\n",
    "        return result\n",
    "\n",
    "\n",
    "# ── Run ID and file setup ──\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_id = f\"{baseline_label}_{MODEL_KEY}_{timestamp}\"\n",
    "\n",
    "output_dir = Path(\"../experiments/results\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "intermediate_path = output_dir / f\"{run_id}_intermediate.jsonl\"\n",
    "\n",
    "print(f\"Run ID:       {run_id}\")\n",
    "print(f\"Intermediate: {intermediate_path}\")\n",
    "\n",
    "# ── Resume: load existing completed samples ──\n",
    "results = []\n",
    "completed_ids = set()\n",
    "if intermediate_path.exists():\n",
    "    with open(intermediate_path) as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                rec = json.loads(line)\n",
    "                completed_ids.add(rec[\"sample_id\"])\n",
    "                results.append(rec)\n",
    "    print(f\"Resuming:     {len(completed_ids)} samples already completed\")\n",
    "print()\n",
    "\n",
    "# ── Diagnostics tracker ──\n",
    "diagnostics = ModelDiagnostics(experiment_id=run_id)\n",
    "\n",
    "# ── Extraction loop ──\n",
    "total = len(selected)\n",
    "start_time = time.time()\n",
    "\n",
    "for i, sample in enumerate(selected):\n",
    "    if sample.id in completed_ids:\n",
    "        print(f\"[{i+1}/{total}] {sample.category} — SKIPPED (already done)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[{i+1}/{total}] {sample.category} ({sample.tier})...\", end=\" \", flush=True)\n",
    "\n",
    "    try:\n",
    "        # 1. Build prompt (baseline-specific)\n",
    "        system_prompt, user_message = build_messages(sample, BASELINE_TYPE)\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # 2. Call model directly (captures raw response + token usage)\n",
    "        raw_response, usage = await model_invoke(\n",
    "            model_key=MODEL_KEY,\n",
    "            messages=messages,\n",
    "            system=system_prompt,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            diagnostics=diagnostics,\n",
    "            agent_name=baseline_label,\n",
    "            category=sample.category,\n",
    "        )\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        # 3. Parse response (baseline-specific)\n",
    "        result = parse_response(raw_response, sample.category, BASELINE_TYPE)\n",
    "\n",
    "        # 4. Evaluate\n",
    "        predicted_text = \" \".join(result.extracted_clauses)\n",
    "        has_prediction = len(result.extracted_clauses) > 0\n",
    "\n",
    "        if sample.has_clause:\n",
    "            if has_prediction:\n",
    "                covers = any(\n",
    "                    span_overlap(predicted_text, gt)\n",
    "                    for gt in sample.ground_truth_spans\n",
    "                )\n",
    "                classification = \"TP\" if covers else \"FN\"\n",
    "            else:\n",
    "                classification = \"FN\"\n",
    "        else:\n",
    "            classification = \"FP\" if has_prediction else \"TN\"\n",
    "\n",
    "        jacc = (\n",
    "            compute_jaccard(predicted_text, sample.ground_truth)\n",
    "            if sample.has_clause and has_prediction\n",
    "            else (1.0 if not sample.has_clause and not has_prediction else 0.0)\n",
    "        )\n",
    "        grounding = (\n",
    "            compute_grounding_rate(result.extracted_clauses, sample.contract_text)\n",
    "            if has_prediction else 1.0\n",
    "        )\n",
    "\n",
    "        # 5. Build full traceable record\n",
    "        record = {\n",
    "            \"sample_id\": sample.id,\n",
    "            \"run_id\": run_id,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"model_key\": MODEL_KEY,\n",
    "            \"model_id\": config.model_id,\n",
    "            \"baseline_type\": BASELINE_TYPE,\n",
    "            \"baseline_label\": baseline_label,\n",
    "            \"category\": sample.category,\n",
    "            \"tier\": sample.tier,\n",
    "            \"contract_title\": sample.contract_title,\n",
    "            \"contract_chars\": len(sample.contract_text),\n",
    "            \"input\": {\n",
    "                \"system_prompt\": system_prompt,\n",
    "                \"user_message_length\": len(user_message),\n",
    "                \"question\": sample.question,\n",
    "            },\n",
    "            \"output\": {\n",
    "                \"raw_response\": raw_response,\n",
    "                \"parsed_clauses\": result.extracted_clauses,\n",
    "                \"num_clauses\": len(result.extracted_clauses),\n",
    "                \"reasoning\": result.reasoning,\n",
    "                \"confidence\": result.confidence,\n",
    "            },\n",
    "            \"ground_truth\": {\n",
    "                \"has_clause\": sample.has_clause,\n",
    "                \"spans\": sample.ground_truth_spans,\n",
    "                \"full_text\": sample.ground_truth,\n",
    "                \"num_spans\": sample.num_spans,\n",
    "            },\n",
    "            \"evaluation\": {\n",
    "                \"classification\": classification,\n",
    "                \"jaccard\": jacc,\n",
    "                \"grounding_rate\": grounding,\n",
    "            },\n",
    "            \"usage\": {\n",
    "                \"input_tokens\": usage.input_tokens,\n",
    "                \"output_tokens\": usage.output_tokens,\n",
    "                \"cache_read_tokens\": getattr(usage, \"cache_read_tokens\", 0),\n",
    "                \"cache_creation_tokens\": getattr(usage, \"cache_creation_tokens\", 0),\n",
    "                \"latency_s\": round(elapsed, 2),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # 6. Append to JSONL immediately (crash-safe)\n",
    "        with open(intermediate_path, \"a\") as f:\n",
    "            f.write(json.dumps(record, default=str) + \"\\n\")\n",
    "\n",
    "        results.append(record)\n",
    "        print(f\"-> {classification} | {len(result.extracted_clauses)} clause(s) | J={jacc:.3f} | {elapsed:.1f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"-> ERROR: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nCompleted: {len(results)} total ({len(completed_ids)} resumed)\")\n",
    "print(f\"Intermediate saved to: {intermediate_path}\")\n",
    "print(f\"Total wall time: {total_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd83a15",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics\n",
    "\n",
    "Metrics follow ContractEval definitions:\n",
    "- **TP**: Label not empty AND prediction covers ground truth span\n",
    "- **TN**: Label empty AND model predicts nothing / \"no related clause\"\n",
    "- **FP**: Label empty BUT model predicts non-empty clause\n",
    "- **FN**: Label not empty BUT model misses (no prediction or doesn't cover span)\n",
    "- **Laziness**: FN where model produced 0 clauses (said \"no related clause\" when one exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da565fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  B1 zero_shot — claude-sonnet-4\n",
      "============================================================\n",
      "  Samples:       9\n",
      "  TP: 5  FP: 0  FN: 1  TN: 3\n",
      "\n",
      "  Precision:     1.000\n",
      "  Recall:        0.833\n",
      "  F1:            0.909\n",
      "  F2:            0.862\n",
      "  Avg Jaccard:   0.401\n",
      "  Laziness rate: 16.7% (1/6)\n",
      "\n",
      "  ContractEval reference (GPT-4.1):\n",
      "  F1=0.641  F2=0.678  Jaccard=0.472  Laziness=7.1%\n",
      "\n",
      "======================================================================\n",
      "  Per-Tier Breakdown\n",
      "======================================================================\n",
      "  Tier         TP   FP   FN   TN      F1      F2  Jaccard\n",
      "  ------------------------------------------------------------\n",
      "  common        1    0    1    1   0.667   0.556    0.060\n",
      "  moderate      2    0    0    1   1.000   1.000    0.754\n",
      "  rare          2    0    0    1   1.000   1.000    0.389\n"
     ]
    }
   ],
   "source": [
    "from src.evaluation.metrics import compute_f1, compute_f2, compute_precision, compute_recall\n",
    "\n",
    "tp = sum(1 for r in results if r[\"evaluation\"][\"classification\"] == \"TP\")\n",
    "fp = sum(1 for r in results if r[\"evaluation\"][\"classification\"] == \"FP\")\n",
    "fn = sum(1 for r in results if r[\"evaluation\"][\"classification\"] == \"FN\")\n",
    "tn = sum(1 for r in results if r[\"evaluation\"][\"classification\"] == \"TN\")\n",
    "\n",
    "total_positive = tp + fn\n",
    "laziness_count = sum(\n",
    "    1 for r in results\n",
    "    if r[\"evaluation\"][\"classification\"] == \"FN\"\n",
    "    and r[\"output\"][\"num_clauses\"] == 0\n",
    ")\n",
    "\n",
    "precision = compute_precision(tp, fp)\n",
    "recall = compute_recall(tp, fn)\n",
    "f1 = compute_f1(tp, fp, fn)\n",
    "f2 = compute_f2(tp, fp, fn)\n",
    "\n",
    "jaccard_scores = [r[\"evaluation\"][\"jaccard\"] for r in results if r[\"ground_truth\"][\"has_clause\"]]\n",
    "avg_jaccard = sum(jaccard_scores) / len(jaccard_scores) if jaccard_scores else 0\n",
    "laziness_rate = laziness_count / total_positive if total_positive > 0 else 0\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  {BASELINE_TYPE} {baseline_label} — {MODEL_KEY}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Samples:       {len(results)}\")\n",
    "print(f\"  TP: {tp}  FP: {fp}  FN: {fn}  TN: {tn}\")\n",
    "print()\n",
    "print(f\"  Precision:     {precision:.3f}\")\n",
    "print(f\"  Recall:        {recall:.3f}\")\n",
    "print(f\"  F1:            {f1:.3f}\")\n",
    "print(f\"  F2:            {f2:.3f}\")\n",
    "print(f\"  Avg Jaccard:   {avg_jaccard:.3f}\")\n",
    "print(f\"  Laziness rate: {laziness_rate:.1%} ({laziness_count}/{total_positive})\")\n",
    "print()\n",
    "print(f\"  ContractEval reference (GPT-4.1):\")\n",
    "print(f\"  F1=0.641  F2=0.678  Jaccard=0.472  Laziness=7.1%\")\n",
    "\n",
    "# Per-tier breakdown\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"  Per-Tier Breakdown\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  {'Tier':<10} {'TP':>4} {'FP':>4} {'FN':>4} {'TN':>4} {'F1':>7} {'F2':>7} {'Jaccard':>8}\")\n",
    "print(f\"  {'-'*60}\")\n",
    "\n",
    "for tier in [\"common\", \"moderate\", \"rare\"]:\n",
    "    tr = [r for r in results if r[\"tier\"] == tier]\n",
    "    t_tp = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"TP\")\n",
    "    t_fp = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"FP\")\n",
    "    t_fn = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"FN\")\n",
    "    t_tn = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"TN\")\n",
    "    t_f1 = compute_f1(t_tp, t_fp, t_fn)\n",
    "    t_f2 = compute_f2(t_tp, t_fp, t_fn)\n",
    "    t_jaccs = [r[\"evaluation\"][\"jaccard\"] for r in tr if r[\"ground_truth\"][\"has_clause\"]]\n",
    "    t_jacc = sum(t_jaccs) / len(t_jaccs) if t_jaccs else 0\n",
    "    print(f\"  {tier:<10} {t_tp:>4} {t_fp:>4} {t_fn:>4} {t_tn:>4} {t_f1:>7.3f} {t_f2:>7.3f} {t_jacc:>8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e42bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"  Per-Sample Results\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    cls = r[\"evaluation\"][\"classification\"]\n",
    "    ok = cls in (\"TP\", \"TN\")\n",
    "\n",
    "    print(f\"\\n  [{i+1}] {'PASS' if ok else 'FAIL'} {cls} | {r['category']} ({r['tier']})\")\n",
    "    print(f\"      Contract: {r['contract_title'][:60]}\")\n",
    "    print(f\"      Question: {r['input']['question'][:80]}...\")\n",
    "\n",
    "    if r[\"ground_truth\"][\"has_clause\"]:\n",
    "        gt = r[\"ground_truth\"][\"full_text\"][:120]\n",
    "        print(f\"      GT:   {gt}...\")\n",
    "\n",
    "    if r[\"output\"][\"num_clauses\"] > 0:\n",
    "        pred = r[\"output\"][\"parsed_clauses\"][0][:120]\n",
    "        print(f\"      Pred: {pred}...\")\n",
    "    else:\n",
    "        print(f\"      Pred: (no clause extracted)\")\n",
    "\n",
    "    print(f\"      Jaccard: {r['evaluation']['jaccard']:.3f} | \"\n",
    "          f\"Grounding: {r['evaluation']['grounding_rate']:.1%} | \"\n",
    "          f\"Tokens: {r['usage']['input_tokens']:,} in / {r['usage']['output_tokens']:,} out | \"\n",
    "          f\"Time: {r['usage']['latency_s']:.1f}s\")\n",
    "\n",
    "    # Raw response preview\n",
    "    raw = r[\"output\"][\"raw_response\"][:200].replace(\"\\n\", \" \")\n",
    "    print(f\"      Raw:  {raw}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0de20",
   "metadata": {},
   "source": [
    "## 4. Model Diagnostics & Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f2e507f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Diagnostics (claude-sonnet-4)\n",
      "==================================================\n",
      "API calls:       9\n",
      "Success rate:    100%\n",
      "Input tokens:    51,996\n",
      "Output tokens:   787\n",
      "Total tokens:    52,783\n",
      "Estimated cost:  $0.1678\n",
      "Avg latency:     2567 ms\n",
      "Total time:      30.5 s\n",
      "\n",
      "Avg tokens/call: 5,777 in / 87 out\n"
     ]
    }
   ],
   "source": [
    "diag_summary = diagnostics.summary()\n",
    "\n",
    "print(f\"Model Diagnostics ({MODEL_KEY})\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"API calls:       {diag_summary['total_calls']}\")\n",
    "print(f\"Success rate:    {diag_summary['success_rate']:.0%}\")\n",
    "print(f\"Input tokens:    {diag_summary['total_input_tokens']:,}\")\n",
    "print(f\"Output tokens:   {diag_summary['total_output_tokens']:,}\")\n",
    "print(f\"Total tokens:    {diag_summary['total_tokens']:,}\")\n",
    "print(f\"Estimated cost:  ${diag_summary['total_cost_usd']:.4f}\")\n",
    "print(f\"Avg latency:     {diag_summary['avg_latency_ms']:.0f} ms\")\n",
    "print(f\"Total time:      {diag_summary['duration_seconds']:.1f} s\")\n",
    "\n",
    "if diag_summary[\"total_calls\"] > 0:\n",
    "    avg_in = diag_summary[\"total_input_tokens\"] / diag_summary[\"total_calls\"]\n",
    "    avg_out = diag_summary[\"total_output_tokens\"] / diag_summary[\"total_calls\"]\n",
    "    print(f\"\\nAvg tokens/call: {avg_in:,.0f} in / {avg_out:,.0f} out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e51d3",
   "metadata": {},
   "source": [
    "## 5. Save Summary\n",
    "\n",
    "Saves three files:\n",
    "1. **Intermediate JSONL** — one full record per sample (already saved during extraction)\n",
    "2. **Summary JSON** — config, prompt, aggregate metrics, per-tier, compact per-sample view\n",
    "3. **Diagnostics JSON** — raw API call log from ModelDiagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65978f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved:      ../experiments/results/zero_shot_claude-sonnet-4_20260215_233422_summary.json\n",
      "Diagnostics saved:  ../experiments/diagnostics/zero_shot_claude-sonnet-4_20260215_233422_diagnostics.json\n",
      "Intermediate saved: ../experiments/results/zero_shot_claude-sonnet-4_20260215_233422_intermediate.jsonl\n",
      "\n",
      "To inspect a single record:\n",
      "  head -1 ../experiments/results/zero_shot_claude-sonnet-4_20260215_233422_intermediate.jsonl | python -m json.tool\n"
     ]
    }
   ],
   "source": [
    "summary = {\n",
    "    \"run_id\": run_id,\n",
    "    \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "    \"config\": {\n",
    "        \"model_key\": MODEL_KEY,\n",
    "        \"model_id\": config.model_id,\n",
    "        \"provider\": config.provider.value,\n",
    "        \"baseline_type\": BASELINE_TYPE,\n",
    "        \"baseline_label\": baseline_label,\n",
    "        \"samples_per_tier\": SAMPLES_PER_TIER,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"max_contract_chars\": MAX_CONTRACT_CHARS,\n",
    "        \"include_negative\": INCLUDE_NEGATIVE_SAMPLES,\n",
    "    },\n",
    "    \"prompt\": {\n",
    "        \"system_prompt\": results[0][\"input\"][\"system_prompt\"] if results else None,\n",
    "        \"template_name\": baseline_label,\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2,\n",
    "        \"avg_jaccard\": avg_jaccard,\n",
    "        \"laziness_rate\": laziness_rate,\n",
    "        \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn,\n",
    "    },\n",
    "    \"per_tier\": {},\n",
    "    \"samples\": [],\n",
    "    \"diagnostics\": diag_summary,\n",
    "    \"intermediate_file\": str(intermediate_path),\n",
    "}\n",
    "\n",
    "# Per-tier metrics\n",
    "for tier in [\"common\", \"moderate\", \"rare\"]:\n",
    "    tr = [r for r in results if r[\"tier\"] == tier]\n",
    "    t_tp = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"TP\")\n",
    "    t_fp = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"FP\")\n",
    "    t_fn = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"FN\")\n",
    "    t_tn = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"TN\")\n",
    "    t_jaccs = [r[\"evaluation\"][\"jaccard\"] for r in tr if r[\"ground_truth\"][\"has_clause\"]]\n",
    "    summary[\"per_tier\"][tier] = {\n",
    "        \"tp\": t_tp, \"fp\": t_fp, \"fn\": t_fn, \"tn\": t_tn,\n",
    "        \"f1\": compute_f1(t_tp, t_fp, t_fn),\n",
    "        \"f2\": compute_f2(t_tp, t_fp, t_fn),\n",
    "        \"avg_jaccard\": sum(t_jaccs) / len(t_jaccs) if t_jaccs else 0,\n",
    "    }\n",
    "\n",
    "# Compact per-sample view (full data is in intermediate JSONL)\n",
    "for r in results:\n",
    "    summary[\"samples\"].append({\n",
    "        \"id\": r[\"sample_id\"],\n",
    "        \"category\": r[\"category\"],\n",
    "        \"tier\": r[\"tier\"],\n",
    "        \"classification\": r[\"evaluation\"][\"classification\"],\n",
    "        \"jaccard\": r[\"evaluation\"][\"jaccard\"],\n",
    "        \"grounding_rate\": r[\"evaluation\"][\"grounding_rate\"],\n",
    "        \"num_clauses_predicted\": r[\"output\"][\"num_clauses\"],\n",
    "        \"num_gt_spans\": r[\"ground_truth\"][\"num_spans\"],\n",
    "        \"input_tokens\": r[\"usage\"][\"input_tokens\"],\n",
    "        \"output_tokens\": r[\"usage\"][\"output_tokens\"],\n",
    "        \"latency_s\": r[\"usage\"][\"latency_s\"],\n",
    "    })\n",
    "\n",
    "# Save summary\n",
    "summary_path = output_dir / f\"{run_id}_summary.json\"\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "print(f\"Summary saved:      {summary_path}\")\n",
    "\n",
    "# Save diagnostics\n",
    "diag_dir = Path(\"../experiments/diagnostics\")\n",
    "diag_dir.mkdir(parents=True, exist_ok=True)\n",
    "diag_path = diag_dir / f\"{run_id}_diagnostics.json\"\n",
    "diagnostics.export(diag_path)\n",
    "print(f\"Diagnostics saved:  {diag_path}\")\n",
    "\n",
    "# Remind about intermediate\n",
    "print(f\"Intermediate saved: {intermediate_path}\")\n",
    "print(f\"\\nTo inspect a single record:\")\n",
    "print(f\"  head -1 {intermediate_path} | python -m json.tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad793b6",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Switch model** — change `MODEL_KEY` in the config cell:\n",
    "```python\n",
    "# Local models (Ollama)\n",
    "MODEL_KEY = \"qwen3-4b\"          # Qwen3 4B\n",
    "MODEL_KEY = \"qwen3-8b\"          # Qwen3 8B\n",
    "MODEL_KEY = \"llama-3.1-8b\"      # LLaMA 3.1 8B\n",
    "\n",
    "# Proprietary (need API keys)\n",
    "MODEL_KEY = \"claude-sonnet-4\"    # Claude Sonnet 4\n",
    "MODEL_KEY = \"gpt-4.1\"           # GPT 4.1\n",
    "MODEL_KEY = \"gpt-4.1-mini\"      # GPT 4.1 Mini\n",
    "MODEL_KEY = \"gemini-2.5-pro\"    # Gemini 2.5 Pro\n",
    "```\n",
    "\n",
    "**Switch baseline** — change `BASELINE_TYPE`:\n",
    "```python\n",
    "BASELINE_TYPE = \"B1\"  # Zero-shot (ContractEval replication)\n",
    "BASELINE_TYPE = \"B4\"  # Chain-of-Thought\n",
    "BASELINE_TYPE = \"M6\"  # Combined specialist prompts\n",
    "```\n",
    "\n",
    "**Scale up** — increase `SAMPLES_PER_TIER` or run full test set via `scripts/run_experiment.py`.\n",
    "\n",
    "**Output files:**\n",
    "- `experiments/results/{run_id}_intermediate.jsonl` — full per-sample records\n",
    "- `experiments/results/{run_id}_summary.json` — config + metrics + compact results\n",
    "- `experiments/diagnostics/{run_id}_diagnostics.json` — raw API call log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
