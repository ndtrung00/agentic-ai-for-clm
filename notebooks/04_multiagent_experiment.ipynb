{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Multi-Agent Experiment (M1–M6)\n",
    "\n",
    "This notebook runs **multi-agent and ablation configurations** on the same stratified\n",
    "CUAD sample used in `03_baseline_calibration.ipynb`, then performs statistical\n",
    "comparison against baselines (B1/B4).\n",
    "\n",
    "**Configurations:**\n",
    "- `M1`: Full multi-agent system (orchestrator + 3 specialists + validation via LangGraph)\n",
    "- `M2`–`M5`: Reserved for ablation studies (not yet implemented)\n",
    "- `M6`: Combined specialist prompts in a single agent (critical ablation: architecture vs prompting)\n",
    "\n",
    "**Key hypotheses tested here:**\n",
    "\n",
    "| ID | Hypothesis | Test |\n",
    "|----|-----------|------|\n",
    "| H1 | Multi-agent beats single-agent baselines | F2(M1) > F2(B1), McNemar p < 0.05 |\n",
    "| H2 | Specialists help rare categories most | ΔF2_rare > ΔF2_common |\n",
    "| H3 | Architecture matters, not just prompts | M1 > M6 significantly |\n",
    "| H4 | Multi-agent produces auditable reasoning | Trace completeness > 90% |\n",
    "\n",
    "**Pipeline:** Same as notebook 03 (crash-safe JSONL, resume support, full traceability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "MODEL_KEY = \"claude-sonnet-4\"       # Model key (see src/models/config.py)\n",
    "EXPERIMENT_TYPE = \"M1\"              # M1=full-multiagent, M6=combined-prompts, M2-M5=reserved\n",
    "SAMPLES_PER_TIER = 5               # Must match baseline runs for fair comparison\n",
    "INCLUDE_NEGATIVE_SAMPLES = True\n",
    "MAX_CONTRACT_CHARS = 100_000\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS = 4096\n",
    "\n",
    "# Path to baseline results for statistical comparison\n",
    "BASELINE_RESULTS_DIR = \"../experiments/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:      Claude Sonnet 4 (claude-sonnet-4-20250514)\n",
      "Provider:   anthropic\n",
      "Experiment: M1 (multiagent)\n",
      "Context:    200,000 tokens\n",
      "API key:    set\n"
     ]
    }
   ],
   "source": [
    "import sys, os, time, json, datetime\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "from src.models.config import get_model_config, ModelProvider\n",
    "\n",
    "config = get_model_config(MODEL_KEY)\n",
    "\n",
    "experiment_labels = {\n",
    "    \"M1\": \"multiagent\",\n",
    "    \"M2\": \"ablation_no_validation\",\n",
    "    \"M3\": \"ablation_single_specialist\",\n",
    "    \"M4\": \"ablation_no_routing\",\n",
    "    \"M5\": \"ablation_no_specialist_prompts\",\n",
    "    \"M6\": \"combined_prompts\",\n",
    "}\n",
    "assert EXPERIMENT_TYPE in experiment_labels, (\n",
    "    f\"Unknown EXPERIMENT_TYPE={EXPERIMENT_TYPE!r}. \"\n",
    "    f\"Valid options: {list(experiment_labels)}\"\n",
    ")\n",
    "experiment_label = experiment_labels[EXPERIMENT_TYPE]\n",
    "\n",
    "print(f\"Model:      {config.name} ({config.model_id})\")\n",
    "print(f\"Provider:   {config.provider.value}\")\n",
    "print(f\"Experiment: {EXPERIMENT_TYPE} ({experiment_label})\")\n",
    "print(f\"Context:    {config.context_window:,} tokens\")\n",
    "\n",
    "# Verify provider connectivity\n",
    "if config.provider == ModelProvider.OLLAMA:\n",
    "    import urllib.request\n",
    "    try:\n",
    "        urllib.request.urlopen(f\"{config.base_url or 'http://localhost:11434/v1'}/models\")\n",
    "        print(\"Ollama:     connected\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING:    Ollama not reachable — {e}\")\n",
    "elif config.provider == ModelProvider.ANTHROPIC:\n",
    "    assert os.getenv(\"ANTHROPIC_API_KEY\"), \"ANTHROPIC_API_KEY not set\"\n",
    "    print(\"API key:    set\")\n",
    "elif config.provider == ModelProvider.OPENAI:\n",
    "    assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not set\"\n",
    "    print(\"API key:    set\")\n",
    "elif config.provider == ModelProvider.GOOGLE:\n",
    "    assert os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\"), \"GEMINI_API_KEY not set\"\n",
    "    print(\"API key:    set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4f5a6",
   "metadata": {},
   "source": [
    "## 1. Load and Sample CUAD Data\n",
    "\n",
    "Identical stratified sampling as notebook 03 (`random.seed(42)`, same tier counts)\n",
    "to ensure fair comparison against baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7c8d9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 20,910\n",
      "Contracts:     510\n",
      "\n",
      "common    : 2,574 samples (2278 pos, 296 neg)\n",
      "moderate  : 7,722 samples (2050 pos, 5672 neg)\n",
      "rare      : 7,293 samples (812 pos, 6481 neg)\n"
     ]
    }
   ],
   "source": [
    "from src.data.cuad_loader import CUADDataLoader, CATEGORY_TIERS\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "loader = CUADDataLoader()\n",
    "loader.load()\n",
    "all_samples = list(loader)\n",
    "\n",
    "print(f\"Total samples: {len(all_samples):,}\")\n",
    "print(f\"Contracts:     {len(loader.get_contracts())}\")\n",
    "print()\n",
    "\n",
    "by_tier: dict[str, list] = defaultdict(list)\n",
    "for s in all_samples:\n",
    "    if len(s.contract_text) <= MAX_CONTRACT_CHARS:\n",
    "        by_tier[s.tier].append(s)\n",
    "\n",
    "for tier in [\"common\", \"moderate\", \"rare\"]:\n",
    "    pos = sum(1 for s in by_tier[tier] if s.has_clause)\n",
    "    neg = len(by_tier[tier]) - pos\n",
    "    print(f\"{tier:10s}: {len(by_tier[tier]):,} samples ({pos} pos, {neg} neg)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1a2b3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 21 samples:\n",
      "\n",
      "  [common  ] Agreement Date                           (1 spans) | 32,321 chars\n",
      "  [common  ] Expiration Date                          (1 spans) | 6,341 chars\n",
      "  [common  ] Parties                                  (7 spans) | 62,272 chars\n",
      "  [common  ] Document Name                            (1 spans) | 48,253 chars\n",
      "  [common  ] Parties                                  (4 spans) | 29,724 chars\n",
      "  [common  ] Effective Date                           (no clause) | 13,330 chars\n",
      "  [common  ] Effective Date                           (no clause) | 27,714 chars\n",
      "  [moderate] Change Of Control                        (2 spans) | 45,846 chars\n",
      "  [moderate] Renewal Term                             (2 spans) | 87,014 chars\n",
      "  [moderate] Post-Termination Services                (1 spans) | 22,288 chars\n",
      "  [moderate] Anti-Assignment                          (1 spans) | 73,890 chars\n",
      "  [moderate] Audit Rights                             (2 spans) | 18,069 chars\n",
      "  [moderate] Renewal Term                             (no clause) | 15,298 chars\n",
      "  [moderate] Irrevocable Or Perpetual License         (no clause) | 5,713 chars\n",
      "  [rare    ] Price Restrictions                       (1 spans) | 39,863 chars\n",
      "  [rare    ] Affiliate License-Licensee               (1 spans) | 24,223 chars\n",
      "  [rare    ] Most Favored Nation                      (1 spans) | 49,191 chars\n",
      "  [rare    ] Competitive Restriction Exception        (1 spans) | 40,856 chars\n",
      "  [rare    ] Price Restrictions                       (3 spans) | 45,234 chars\n",
      "  [rare    ] Non-Disparagement                        (no clause) | 63,439 chars\n",
      "  [rare    ] Most Favored Nation                      (no clause) | 10,217 chars\n"
     ]
    }
   ],
   "source": [
    "selected = []\n",
    "for tier in [\"common\", \"moderate\", \"rare\"]:\n",
    "    tier_samples = by_tier[tier]\n",
    "    positive = [s for s in tier_samples if s.has_clause]\n",
    "    negative = [s for s in tier_samples if not s.has_clause]\n",
    "\n",
    "    n_pos = min(SAMPLES_PER_TIER, len(positive))\n",
    "    selected.extend(random.sample(positive, n_pos))\n",
    "\n",
    "    if INCLUDE_NEGATIVE_SAMPLES and negative:\n",
    "        n_neg = min(max(1, SAMPLES_PER_TIER // 2), len(negative))\n",
    "        selected.extend(random.sample(negative, n_neg))\n",
    "\n",
    "print(f\"Selected {len(selected)} samples:\\n\")\n",
    "for s in selected:\n",
    "    info = f\"{s.num_spans} spans\" if s.has_clause else \"no clause\"\n",
    "    print(f\"  [{s.tier:8s}] {s.category:40s} ({info}) | {len(s.contract_text):,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8",
   "metadata": {},
   "source": [
    "## 2. Run Extraction\n",
    "\n",
    "Extraction path depends on `EXPERIMENT_TYPE`:\n",
    "\n",
    "- **M1**: Orchestrator routes to specialist agent (risk/temporal/IP) via LangGraph,\n",
    "  then validation layer checks grounding. Full trace captured for H4.\n",
    "- **M6**: Single agent with combined specialist prompts (all domain knowledge in one prompt).\n",
    "  Tests whether multi-agent architecture provides benefit beyond prompt engineering.\n",
    "- **M2–M5**: Reserved for ablation studies (raise error if selected).\n",
    "\n",
    "Same crash-safe JSONL + resume logic as notebook 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9e0f1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1 Orchestrator ready with 3 specialists (validation=None)\n",
      "Run ID:       multiagent_claude-sonnet-4_20260216_005035\n",
      "Intermediate: ../experiments/results/multiagent_claude-sonnet-4_20260216_005035_intermediate.jsonl\n",
      "\n",
      "[1/21] Agreement Date (common)... -> FN | 1 clause(s) | J=0.078 | 9.2s\n",
      "[2/21] Expiration Date (common)... -> TP | 1 clause(s) | J=0.929 | 7.9s\n",
      "[3/21] Parties (common)... -> TP | 4 clause(s) | J=0.034 | 14.8s\n",
      "[4/21] Document Name (common)... -> TP | 2 clause(s) | J=0.069 | 9.1s\n",
      "[5/21] Parties (common)... -> TP | 8 clause(s) | J=0.017 | 15.3s\n",
      "[6/21] Effective Date (common)... -> FP | 1 clause(s) | J=0.000 | 8.3s\n",
      "[7/21] Effective Date (common)... -> FP | 3 clause(s) | J=0.000 | 9.1s\n",
      "[8/21] Change Of Control (moderate)... -> FN | 0 clause(s) | J=0.000 | 0.0s\n",
      "[9/21] Renewal Term (moderate)... -> TP | 1 clause(s) | J=0.628 | 17.4s\n",
      "[10/21] Post-Termination Services (moderate)... -> TP | 3 clause(s) | J=0.635 | 11.8s\n",
      "[11/21] Anti-Assignment (moderate)... -> TP | 1 clause(s) | J=0.380 | 9.1s\n",
      "[12/21] Audit Rights (moderate)... -> TP | 3 clause(s) | J=0.344 | 7.9s\n",
      "[13/21] Renewal Term (moderate)... -> FP | 2 clause(s) | J=0.000 | 5.5s\n",
      "[14/21] Irrevocable Or Perpetual License (moderate)... -> TN | 0 clause(s) | J=1.000 | 5.6s\n",
      "[15/21] Price Restrictions (rare)... -> FN | 2 clause(s) | J=0.691 | 7.6s\n",
      "[16/21] Affiliate License-Licensee (rare)... -> TP | 2 clause(s) | J=0.535 | 10.4s\n",
      "[17/21] Most Favored Nation (rare)... -> FN | 2 clause(s) | J=0.440 | 12.6s\n",
      "[18/21] Competitive Restriction Exception (rare)... -> TP | 2 clause(s) | J=0.225 | 13.3s\n",
      "[19/21] Price Restrictions (rare)... -> FN | 0 clause(s) | J=0.000 | 10.3s\n",
      "[20/21] Non-Disparagement (rare)... -> TN | 0 clause(s) | J=1.000 | 9.5s\n",
      "[21/21] Most Favored Nation (rare)... -> TN | 0 clause(s) | J=1.000 | 6.3s\n",
      "\n",
      "Completed: 21 total (0 resumed)\n",
      "Intermediate saved to: ../experiments/results/multiagent_claude-sonnet-4_20260216_005035_intermediate.jsonl\n",
      "Total wall time: 201.1s\n"
     ]
    }
   ],
   "source": [
    "from src.models import invoke_model as model_invoke\n",
    "from src.models.diagnostics import ModelDiagnostics, TokenUsage\n",
    "from src.evaluation.metrics import span_overlap, compute_jaccard, compute_grounding_rate\n",
    "\n",
    "# ── M1: Multi-Agent Setup ──\n",
    "_orchestrator = None\n",
    "_m1_diagnostics = None\n",
    "\n",
    "if EXPERIMENT_TYPE == \"M1\":\n",
    "    from src.agents.base import AgentConfig\n",
    "    from src.agents import (\n",
    "        Orchestrator,\n",
    "        RiskLiabilityAgent,\n",
    "        TemporalRenewalAgent,\n",
    "        IPCommercialAgent,\n",
    "    )\n",
    "    _m1_diagnostics = ModelDiagnostics()\n",
    "    _risk_config = AgentConfig(name=\"risk_liability\", model_key=MODEL_KEY, prompt_name=\"risk_liability\")\n",
    "    _temporal_config = AgentConfig(name=\"temporal_renewal\", model_key=MODEL_KEY, prompt_name=\"temporal_renewal\")\n",
    "    _ip_config = AgentConfig(name=\"ip_commercial\", model_key=MODEL_KEY, prompt_name=\"ip_commercial\")\n",
    "    _specialists = {\n",
    "        \"risk_liability\": RiskLiabilityAgent(config=_risk_config, diagnostics=_m1_diagnostics),\n",
    "        \"temporal_renewal\": TemporalRenewalAgent(config=_temporal_config, diagnostics=_m1_diagnostics),\n",
    "        \"ip_commercial\": IPCommercialAgent(config=_ip_config, diagnostics=_m1_diagnostics),\n",
    "    }\n",
    "    _orchestrator = Orchestrator(\n",
    "        specialists=_specialists,\n",
    "        validation_agent=None,\n",
    "        config=AgentConfig(name=\"orchestrator\", model_key=MODEL_KEY),\n",
    "    )\n",
    "    print(f\"M1 Orchestrator ready with {len(_specialists)} specialists (validation=None)\")\n",
    "\n",
    "# ── M6: Combined Prompts Setup ──\n",
    "_m6_baseline = None\n",
    "\n",
    "if EXPERIMENT_TYPE == \"M6\":\n",
    "    from src.agents.base import AgentConfig\n",
    "    from src.baselines.combined_prompts import COMBINED_PROMPT, CombinedPromptsBaseline\n",
    "    _m6_baseline = CombinedPromptsBaseline(\n",
    "        config=AgentConfig(name=\"combined_prompts\", model_key=MODEL_KEY),\n",
    "        diagnostics=None,  # We create a separate diagnostics below\n",
    "    )\n",
    "    print(f\"M6 Combined Prompts baseline ready\")\n",
    "\n",
    "# ── M2–M5: Not yet implemented ──\n",
    "if EXPERIMENT_TYPE in (\"M2\", \"M3\", \"M4\", \"M5\"):\n",
    "    raise NotImplementedError(\n",
    "        f\"{EXPERIMENT_TYPE} ablation is not yet implemented. \"\n",
    "        f\"Currently available: M1 (full multi-agent), M6 (combined prompts). \"\n",
    "        f\"See CLAUDE.md for planned ablation definitions.\"\n",
    "    )\n",
    "\n",
    "# ── Run ID and file setup ──\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_id = f\"{experiment_label}_{MODEL_KEY}_{timestamp}\"\n",
    "\n",
    "output_dir = Path(\"../experiments/results\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "intermediate_path = output_dir / f\"{run_id}_intermediate.jsonl\"\n",
    "\n",
    "print(f\"Run ID:       {run_id}\")\n",
    "print(f\"Intermediate: {intermediate_path}\")\n",
    "\n",
    "# ── Resume: load existing completed samples ──\n",
    "results = []\n",
    "completed_ids = set()\n",
    "if intermediate_path.exists():\n",
    "    with open(intermediate_path) as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                rec = json.loads(line)\n",
    "                completed_ids.add(rec[\"sample_id\"])\n",
    "                results.append(rec)\n",
    "    print(f\"Resuming:     {len(completed_ids)} samples already completed\")\n",
    "print()\n",
    "\n",
    "# ── Diagnostics tracker ──\n",
    "diagnostics = _m1_diagnostics if EXPERIMENT_TYPE == \"M1\" else ModelDiagnostics(experiment_id=run_id)\n",
    "if _m6_baseline is not None:\n",
    "    _m6_baseline.diagnostics = diagnostics\n",
    "\n",
    "# ── Extraction loop ──\n",
    "total = len(selected)\n",
    "start_time = time.time()\n",
    "\n",
    "for i, sample in enumerate(selected):\n",
    "    if sample.id in completed_ids:\n",
    "        print(f\"[{i+1}/{total}] {sample.category} — SKIPPED (already done)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[{i+1}/{total}] {sample.category} ({sample.tier})...\", end=\" \", flush=True)\n",
    "\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        trace_nodes = []  # For H4 trace completeness\n",
    "\n",
    "        if EXPERIMENT_TYPE == \"M1\":\n",
    "            # ── M1: Use Orchestrator (LangGraph) ──\n",
    "            n_calls_before = len(diagnostics.calls)\n",
    "            result = await _orchestrator.extract(\n",
    "                contract_text=sample.contract_text,\n",
    "                category=sample.category,\n",
    "                question=sample.question,\n",
    "            )\n",
    "            raw_response = result.reasoning\n",
    "            system_prompt = \"M1 multi-agent (orchestrator → specialist → validation)\"\n",
    "            user_message = f\"Category: {sample.category}\\nQuestion: {sample.question}\"\n",
    "\n",
    "            # Aggregate usage from all calls made during this extraction\n",
    "            recent_calls = diagnostics.calls[n_calls_before:]\n",
    "            agg_input = sum(c.usage.input_tokens for c in recent_calls)\n",
    "            agg_output = sum(c.usage.output_tokens for c in recent_calls)\n",
    "            usage = type(\"Usage\", (), {\n",
    "                \"input_tokens\": agg_input,\n",
    "                \"output_tokens\": agg_output,\n",
    "                \"cache_read_tokens\": 0,\n",
    "                \"cache_creation_tokens\": 0,\n",
    "            })()\n",
    "\n",
    "            # Capture trace nodes for H4\n",
    "            trace_nodes = [c.agent_name for c in recent_calls]\n",
    "\n",
    "        elif EXPERIMENT_TYPE == \"M6\":\n",
    "            # ── M6: Combined Prompts (single-agent ablation) ──\n",
    "            from src.baselines.combined_prompts import COMBINED_PROMPT\n",
    "            system_prompt = None\n",
    "            user_message = COMBINED_PROMPT.format(\n",
    "                category=sample.category,\n",
    "                contract_text=sample.contract_text,\n",
    "                question=sample.question,\n",
    "            )\n",
    "            messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "\n",
    "            raw_response, usage = await model_invoke(\n",
    "                model_key=MODEL_KEY,\n",
    "                messages=messages,\n",
    "                system=system_prompt,\n",
    "                temperature=TEMPERATURE,\n",
    "                max_tokens=MAX_TOKENS,\n",
    "                diagnostics=diagnostics,\n",
    "                agent_name=\"combined_prompts\",\n",
    "                category=sample.category,\n",
    "            )\n",
    "\n",
    "            # Parse M6 response (JSON or plaintext fallback)\n",
    "            data = _m6_baseline.parse_json_response(raw_response)\n",
    "            if data and \"extracted_clauses\" in data:\n",
    "                result = _m6_baseline.result_from_dict(data, sample.category)\n",
    "            else:\n",
    "                result = _m6_baseline._parse_plaintext(raw_response, sample.category)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        # 4. Evaluate\n",
    "        predicted_text = \" \".join(result.extracted_clauses)\n",
    "        has_prediction = len(result.extracted_clauses) > 0\n",
    "\n",
    "        if sample.has_clause:\n",
    "            if has_prediction:\n",
    "                covers = any(\n",
    "                    span_overlap(predicted_text, gt)\n",
    "                    for gt in sample.ground_truth_spans\n",
    "                )\n",
    "                classification = \"TP\" if covers else \"FN\"\n",
    "            else:\n",
    "                classification = \"FN\"\n",
    "        else:\n",
    "            classification = \"FP\" if has_prediction else \"TN\"\n",
    "\n",
    "        jacc = (\n",
    "            compute_jaccard(predicted_text, sample.ground_truth)\n",
    "            if sample.has_clause and has_prediction\n",
    "            else (1.0 if not sample.has_clause and not has_prediction else 0.0)\n",
    "        )\n",
    "        grounding = (\n",
    "            compute_grounding_rate(result.extracted_clauses, sample.contract_text)\n",
    "            if has_prediction else 1.0\n",
    "        )\n",
    "\n",
    "        # 5. Build full traceable record\n",
    "        record = {\n",
    "            \"sample_id\": sample.id,\n",
    "            \"run_id\": run_id,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"model_key\": MODEL_KEY,\n",
    "            \"model_id\": config.model_id,\n",
    "            \"experiment_type\": EXPERIMENT_TYPE,\n",
    "            \"experiment_label\": experiment_label,\n",
    "            \"category\": sample.category,\n",
    "            \"tier\": sample.tier,\n",
    "            \"contract_title\": sample.contract_title,\n",
    "            \"contract_chars\": len(sample.contract_text),\n",
    "            \"input\": {\n",
    "                \"system_prompt\": system_prompt,\n",
    "                \"user_message_length\": len(user_message),\n",
    "                \"question\": sample.question,\n",
    "            },\n",
    "            \"output\": {\n",
    "                \"raw_response\": raw_response,\n",
    "                \"parsed_clauses\": result.extracted_clauses,\n",
    "                \"num_clauses\": len(result.extracted_clauses),\n",
    "                \"reasoning\": result.reasoning,\n",
    "                \"confidence\": result.confidence,\n",
    "            },\n",
    "            \"ground_truth\": {\n",
    "                \"has_clause\": sample.has_clause,\n",
    "                \"spans\": sample.ground_truth_spans,\n",
    "                \"full_text\": sample.ground_truth,\n",
    "                \"num_spans\": sample.num_spans,\n",
    "            },\n",
    "            \"evaluation\": {\n",
    "                \"classification\": classification,\n",
    "                \"jaccard\": jacc,\n",
    "                \"grounding_rate\": grounding,\n",
    "            },\n",
    "            \"usage\": {\n",
    "                \"input_tokens\": getattr(usage, \"input_tokens\", 0),\n",
    "                \"output_tokens\": getattr(usage, \"output_tokens\", 0),\n",
    "                \"cache_read_tokens\": getattr(usage, \"cache_read_tokens\", 0),\n",
    "                \"cache_creation_tokens\": getattr(usage, \"cache_creation_tokens\", 0),\n",
    "                \"latency_s\": round(elapsed, 2),\n",
    "            },\n",
    "            \"trace\": {\n",
    "                \"nodes_visited\": trace_nodes,\n",
    "                \"num_llm_calls\": len(trace_nodes) if trace_nodes else 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # 6. Append to JSONL immediately (crash-safe)\n",
    "        with open(intermediate_path, \"a\") as f:\n",
    "            f.write(json.dumps(record, default=str) + \"\\n\")\n",
    "\n",
    "        results.append(record)\n",
    "        print(f\"-> {classification} | {len(result.extracted_clauses)} clause(s) | J={jacc:.3f} | {elapsed:.1f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"-> ERROR: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nCompleted: {len(results)} total ({len(completed_ids)} resumed)\")\n",
    "print(f\"Intermediate saved to: {intermediate_path}\")\n",
    "print(f\"Total wall time: {total_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics\n",
    "\n",
    "Same ContractEval definitions as notebook 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  M1 multiagent — claude-sonnet-4\n",
      "============================================================\n",
      "  Samples:       21\n",
      "  TP: 10  FP: 3  FN: 5  TN: 3\n",
      "\n",
      "  Precision:     0.769\n",
      "  Recall:        0.667\n",
      "  F1:            0.714\n",
      "  F2:            0.685\n",
      "  Avg Jaccard:   0.334\n",
      "  Laziness rate: 13.3% (2/15)\n",
      "\n",
      "  ContractEval reference (GPT-4.1):\n",
      "  F1=0.641  F2=0.678  Jaccard=0.472  Laziness=7.1%\n",
      "\n",
      "======================================================================\n",
      "  Per-Tier Breakdown\n",
      "======================================================================\n",
      "  Tier         TP   FP   FN   TN      F1      F2  Jaccard\n",
      "  ------------------------------------------------------------\n",
      "  common        4    2    1    0   0.727   0.769    0.226\n",
      "  moderate      4    1    1    1   0.800   0.800    0.398\n",
      "  rare          2    0    3    2   0.571   0.455    0.378\n"
     ]
    }
   ],
   "source": [
    "from src.evaluation.metrics import compute_f1, compute_f2, compute_precision, compute_recall\n",
    "\n",
    "tp = sum(1 for r in results if r[\"evaluation\"][\"classification\"] == \"TP\")\n",
    "fp = sum(1 for r in results if r[\"evaluation\"][\"classification\"] == \"FP\")\n",
    "fn = sum(1 for r in results if r[\"evaluation\"][\"classification\"] == \"FN\")\n",
    "tn = sum(1 for r in results if r[\"evaluation\"][\"classification\"] == \"TN\")\n",
    "\n",
    "total_positive = tp + fn\n",
    "laziness_count = sum(\n",
    "    1 for r in results\n",
    "    if r[\"evaluation\"][\"classification\"] == \"FN\"\n",
    "    and r[\"output\"][\"num_clauses\"] == 0\n",
    ")\n",
    "\n",
    "precision = compute_precision(tp, fp)\n",
    "recall = compute_recall(tp, fn)\n",
    "f1 = compute_f1(tp, fp, fn)\n",
    "f2 = compute_f2(tp, fp, fn)\n",
    "\n",
    "jaccard_scores = [r[\"evaluation\"][\"jaccard\"] for r in results if r[\"ground_truth\"][\"has_clause\"]]\n",
    "avg_jaccard = sum(jaccard_scores) / len(jaccard_scores) if jaccard_scores else 0\n",
    "laziness_rate = laziness_count / total_positive if total_positive > 0 else 0\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  {EXPERIMENT_TYPE} {experiment_label} — {MODEL_KEY}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Samples:       {len(results)}\")\n",
    "print(f\"  TP: {tp}  FP: {fp}  FN: {fn}  TN: {tn}\")\n",
    "print()\n",
    "print(f\"  Precision:     {precision:.3f}\")\n",
    "print(f\"  Recall:        {recall:.3f}\")\n",
    "print(f\"  F1:            {f1:.3f}\")\n",
    "print(f\"  F2:            {f2:.3f}\")\n",
    "print(f\"  Avg Jaccard:   {avg_jaccard:.3f}\")\n",
    "print(f\"  Laziness rate: {laziness_rate:.1%} ({laziness_count}/{total_positive})\")\n",
    "print()\n",
    "print(f\"  ContractEval reference (GPT-4.1):\")\n",
    "print(f\"  F1=0.641  F2=0.678  Jaccard=0.472  Laziness=7.1%\")\n",
    "\n",
    "# Per-tier breakdown\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"  Per-Tier Breakdown\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  {'Tier':<10} {'TP':>4} {'FP':>4} {'FN':>4} {'TN':>4} {'F1':>7} {'F2':>7} {'Jaccard':>8}\")\n",
    "print(f\"  {'-'*60}\")\n",
    "\n",
    "for tier in [\"common\", \"moderate\", \"rare\"]:\n",
    "    tr = [r for r in results if r[\"tier\"] == tier]\n",
    "    t_tp = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"TP\")\n",
    "    t_fp = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"FP\")\n",
    "    t_fn = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"FN\")\n",
    "    t_tn = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"TN\")\n",
    "    t_f1 = compute_f1(t_tp, t_fp, t_fn)\n",
    "    t_f2 = compute_f2(t_tp, t_fp, t_fn)\n",
    "    t_jaccs = [r[\"evaluation\"][\"jaccard\"] for r in tr if r[\"ground_truth\"][\"has_clause\"]]\n",
    "    t_jacc = sum(t_jaccs) / len(t_jaccs) if t_jaccs else 0\n",
    "    print(f\"  {tier:<10} {t_tp:>4} {t_fp:>4} {t_fn:>4} {t_tn:>4} {t_f1:>7.3f} {t_f2:>7.3f} {t_jacc:>8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b0c1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "  Per-Sample Results\n",
      "==========================================================================================\n",
      "\n",
      "  [1] FAIL FN | Agreement Date (common)\n",
      "      Contract: TodosMedicalLtd_20190328_20-F_EX-4.10_11587157_EX-4.10_Marke\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Agreement Date\" that s...\n",
      "      GT:   20t h day of December 2018...\n",
      "      Pred: THIS AGREEMENT is made and entered into this 20th day of December 2018 (the \"Effective Date\"), by and between Todos Medi...\n",
      "      Jaccard: 0.078 | Grounding: 0.0% | Tokens: 7,844 in / 387 out | Time: 9.2s\n",
      "      Trace: temporal_renewal\n",
      "\n",
      "  [2] PASS TP | Expiration Date (common)\n",
      "      Contract: Freecook_20180605_S-1_EX-10.3_11233807_EX-10.3_Hosting Agree\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Expiration Date\" that ...\n",
      "      GT:   Terms of the project: 12 weeks from February 8, 2018 to May 3, 2018...\n",
      "      Pred: Terms of the project: 12 weeks from February 8, 2018 to May 3, 2018....\n",
      "      Jaccard: 0.929 | Grounding: 100.0% | Tokens: 1,813 in / 335 out | Time: 7.9s\n",
      "      Trace: temporal_renewal\n",
      "\n",
      "  [3] PASS TP | Parties (common)\n",
      "      Contract: SUCAMPOPHARMACEUTICALS,INC_11_04_2015-EX-10.2-STRATEGIC ALLI\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Parties\" that should b...\n",
      "      GT:   Sucampo Pharmaceuticals, Inc....\n",
      "      Pred: STRATEGIC ALLIANCE AGREEMENT dated as of August 26, 2015 among Sucampo Pharmaceuticals, Inc., Sucampo Pharma, LLC. and R...\n",
      "      Jaccard: 0.034 | Grounding: 100.0% | Tokens: 14,592 in / 709 out | Time: 14.8s\n",
      "      Trace: temporal_renewal\n",
      "\n",
      "  [4] PASS TP | Document Name (common)\n",
      "      Contract: Apollo Endosurgery - Manufacturing and Supply Agreement\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Document Name\" that sh...\n",
      "      GT:   Manufacturing and Supply Agreement...\n",
      "      Pred: MANUFACTURING AND SUPPLY AGREEMENT...\n",
      "      Jaccard: 0.069 | Grounding: 100.0% | Tokens: 11,032 in / 419 out | Time: 9.1s\n",
      "      Trace: temporal_renewal\n",
      "\n",
      "  [5] PASS TP | Parties (common)\n",
      "      Contract: WARNINGMANAGEMENTSERVICESINC_12_10_1999-EX-10-ENDORSEMENT AG\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Parties\" that should b...\n",
      "      GT:   PEY DIRT, INC....\n",
      "      Pred: THIS ENDORSEMENT AGREEMENT (this \"Agreement\"), made and entered into as of May 31, 1999, by and between FAMOUS FIXINS, I...\n",
      "      Jaccard: 0.017 | Grounding: 87.5% | Tokens: 6,946 in / 880 out | Time: 15.2s\n",
      "      Trace: temporal_renewal\n",
      "\n",
      "  [6] FAIL FP | Effective Date (common)\n",
      "      Contract: ScansourceInc_20190822_10-K_EX-10.38_11793958_EX-10.38_Distr\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Effective Date\" that s...\n",
      "      Pred: Term and Termination. This Agreement and the license granted herein shall remain effective until terminated....\n",
      "      Jaccard: 0.000 | Grounding: 100.0% | Tokens: 3,331 in / 365 out | Time: 8.3s\n",
      "      Trace: temporal_renewal\n",
      "\n",
      "  [7] FAIL FP | Effective Date (common)\n",
      "      Contract: SPARKLINGSPRINGWATERHOLDINGSLTD_07_03_2002-EX-10.13-SOFTWARE\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Effective Date\" that s...\n",
      "      Pred: Maintenance Commencement Date means the date that acceptance testing is successfully completed at all of the Authorized ...\n",
      "      Jaccard: 0.000 | Grounding: 100.0% | Tokens: 6,127 in / 384 out | Time: 9.1s\n",
      "      Trace: temporal_renewal\n",
      "\n",
      "  [8] FAIL FN | Change Of Control (moderate)\n",
      "      Contract: Columbia Laboratories, (Bermuda) Ltd. - AMEND NO. 2 TO MANUF\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Change Of Control\" tha...\n",
      "      GT:   (ii) a Change of Control Event with respect to Fleet occurs;...\n",
      "      Pred: (no clause extracted)\n",
      "      Jaccard: 0.000 | Grounding: 100.0% | Tokens: 0 in / 0 out | Time: 0.0s\n",
      "\n",
      "  [9] PASS TP | Renewal Term (moderate)\n",
      "      Contract: StampscomInc_20001114_10-Q_EX-10.47_2631630_EX-10.47_Co-Bran\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Renewal Term\" that sho...\n",
      "      GT:   MBE shall have the right to elect by written notice to the Company at any time between two (2) and six (6) months prior ...\n",
      "      Pred: The initial term of this Agreement shall commence on the Effective Date and end on the fifth anniversary of the Effectiv...\n",
      "      Jaccard: 0.628 | Grounding: 0.0% | Tokens: 18,751 in / 654 out | Time: 17.4s\n",
      "      Trace: temporal_renewal\n",
      "\n",
      "  [10] PASS TP | Post-Termination Services (moderate)\n",
      "      Contract: HealthcareIntegratedTechnologiesInc_20190812_8-K_EX-10.1_117\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Post-Termination Servi...\n",
      "      GT:   In addition to the foregoing, if, at the time of termination of this Agreement, Reseller shall have additional Product u...\n",
      "      Pred: Upon any termination of this Agreement: (i) all rights granted by Supplier to Reseller under the Agreement revert to Sup...\n",
      "      Jaccard: 0.635 | Grounding: 100.0% | Tokens: 5,625 in / 841 out | Time: 11.8s\n",
      "      Trace: risk_liability\n",
      "\n",
      "  [11] PASS TP | Anti-Assignment (moderate)\n",
      "      Contract: SENMIAOTECHNOLOGYLTD_02_19_2019-EX-10.5-Collaboration Agreem\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Anti-Assignment\" that ...\n",
      "      GT:   During the term of this Agreement, neither party may assign, or transfer its rights and obligations under this Agreement...\n",
      "      Pred: 17. Transfer During the term of this Agreement, neither party may assign, or transfer its rights and obligations under t...\n",
      "      Jaccard: 0.380 | Grounding: 100.0% | Tokens: 17,001 in / 468 out | Time: 9.1s\n",
      "      Trace: temporal_renewal\n",
      "\n",
      "  [12] PASS TP | Audit Rights (moderate)\n",
      "      Contract: ArconicRolledProductsCorp_20191217_10-12B_EX-2.7_11923804_EX\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Audit Rights\" that sho...\n",
      "      GT:   Licensor, as owner of the Licensed Mark, shall have the right at all times to control and approve the nature and quality...\n",
      "      Pred: Licensor, as owner of the Licensed Mark, shall have the right at all times to control and approve the nature and quality...\n",
      "      Jaccard: 0.344 | Grounding: 100.0% | Tokens: 4,598 in / 539 out | Time: 7.9s\n",
      "      Trace: risk_liability\n",
      "\n",
      "  [13] FAIL FP | Renewal Term (moderate)\n",
      "      Contract: JINGWEIINTERNATIONALLTD_10_04_2007-EX-10.7-INTELLECTUAL PROP\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Renewal Term\" that sho...\n",
      "      Pred: The term of this Agreement is five (5) years unless the early termination in accordance with this Agreement....\n",
      "      Jaccard: 0.000 | Grounding: 100.0% | Tokens: 4,198 in / 232 out | Time: 5.5s\n",
      "      Trace: temporal_renewal\n",
      "\n",
      "  [14] PASS TN | Irrevocable Or Perpetual License (moderate)\n",
      "      Contract: GRIDIRONBIONUTRIENTS,INC_02_05_2020-EX-10.3-SUPPLY AGREEMENT\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Irrevocable Or Perpetu...\n",
      "      Pred: (no clause extracted)\n",
      "      Jaccard: 1.000 | Grounding: 100.0% | Tokens: 1,798 in / 250 out | Time: 5.6s\n",
      "      Trace: ip_commercial\n",
      "\n",
      "  [15] FAIL FN | Price Restrictions (rare)\n",
      "      Contract: BNLFINANCIALCORP_03_30_2007-EX-10.8-OUTSOURCING AGREEMENT\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Price Restrictions\" th...\n",
      "      GT:   After the initial twelve months of this Agreement, VIP may adjust the rates in Schedule A to VIP's then current standard...\n",
      "      Pred: The minimum monthly fee shall not be less than five thousand dollars ($5,000) per month (as applicable, \"Minimum Fee\")....\n",
      "      Jaccard: 0.691 | Grounding: 50.0% | Tokens: 9,500 in / 347 out | Time: 7.7s\n",
      "      Trace: ip_commercial\n",
      "\n",
      "  [16] PASS TP | Affiliate License-Licensee (rare)\n",
      "      Contract: BizzingoInc_20120322_8-K_EX-10.17_7504499_EX-10.17_Endorseme\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Affiliate License-Lice...\n",
      "      GT:   Subject to the terms and conditions set forth herein, Theismann hereby grants to Bizzingo and its affiliates the unlimit...\n",
      "      Pred: Subject to the terms and conditions set forth herein, Theismann hereby grants to Bizzingo and its affiliates the unlimit...\n",
      "      Jaccard: 0.535 | Grounding: 100.0% | Tokens: 6,214 in / 408 out | Time: 10.4s\n",
      "      Trace: ip_commercial\n",
      "\n",
      "  [17] FAIL FN | Most Favored Nation (rare)\n",
      "      Contract: ChinaRealEstateInformationCorp_20090929_F-1_EX-10.32_4771615\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Most Favored Nation\" t...\n",
      "      GT:   In the event E-House Research and Training Institute becomes entitled to charge, invoice, or otherwise receive from, Lic...\n",
      "      Pred: such fees shall be commercially reasonable and (ii) such fees shall not exceed the fees charged by Licensor to unaffilia...\n",
      "      Jaccard: 0.440 | Grounding: 100.0% | Tokens: 11,829 in / 583 out | Time: 12.6s\n",
      "      Trace: risk_liability\n",
      "\n",
      "  [18] PASS TP | Competitive Restriction Exception (rare)\n",
      "      Contract: STAMPSCOMINC_06_24_1999-EX-10.18-SPONSORSHIP AGREEMENT\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Competitive Restrictio...\n",
      "      GT:   Notwithstanding the above, Intuit may include editorial content or tools      about or from a Client Competitor and incl...\n",
      "      Pred: Throughout the Term Intuit will not place, and will not allow any party acting on its behalf to place, any graphic, link...\n",
      "      Jaccard: 0.225 | Grounding: 100.0% | Tokens: 9,361 in / 592 out | Time: 13.3s\n",
      "      Trace: ip_commercial\n",
      "\n",
      "  [19] FAIL FN | Price Restrictions (rare)\n",
      "      Contract: GpaqAcquisitionHoldingsInc_20200123_S-4A_EX-10.6_11951677_EX\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Price Restrictions\" th...\n",
      "      GT:   To the extent that the Village Media Company and PFHOF work collaboratively on media projects, the EP's services on such...\n",
      "      Pred: (no clause extracted)\n",
      "      Jaccard: 0.000 | Grounding: 100.0% | Tokens: 10,861 in / 454 out | Time: 10.3s\n",
      "      Trace: ip_commercial\n",
      "\n",
      "  [20] PASS TN | Non-Disparagement (rare)\n",
      "      Contract: IMPCOTECHNOLOGIESINC_04_15_2003-EX-10.65-JOINT VENTURE AGREE\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Non-Disparagement\" tha...\n",
      "      Pred: (no clause extracted)\n",
      "      Jaccard: 1.000 | Grounding: 100.0% | Tokens: 15,219 in / 375 out | Time: 9.4s\n",
      "      Trace: risk_liability\n",
      "\n",
      "  [21] PASS TN | Most Favored Nation (rare)\n",
      "      Contract: HerImports_20161018_8-KA_EX-10.14_9765707_EX-10.14_Maintenan\n",
      "      Question: Highlight the parts (if any) of this contract related to \"Most Favored Nation\" t...\n",
      "      Pred: (no clause extracted)\n",
      "      Jaccard: 1.000 | Grounding: 100.0% | Tokens: 2,686 in / 253 out | Time: 6.3s\n",
      "      Trace: risk_liability\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"  Per-Sample Results\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    cls = r[\"evaluation\"][\"classification\"]\n",
    "    ok = cls in (\"TP\", \"TN\")\n",
    "\n",
    "    print(f\"\\n  [{i+1}] {'PASS' if ok else 'FAIL'} {cls} | {r['category']} ({r['tier']})\")\n",
    "    print(f\"      Contract: {r['contract_title'][:60]}\")\n",
    "    print(f\"      Question: {r['input']['question'][:80]}...\")\n",
    "\n",
    "    if r[\"ground_truth\"][\"has_clause\"]:\n",
    "        gt = r[\"ground_truth\"][\"full_text\"][:120]\n",
    "        print(f\"      GT:   {gt}...\")\n",
    "\n",
    "    if r[\"output\"][\"num_clauses\"] > 0:\n",
    "        pred = r[\"output\"][\"parsed_clauses\"][0][:120]\n",
    "        print(f\"      Pred: {pred}...\")\n",
    "    else:\n",
    "        print(f\"      Pred: (no clause extracted)\")\n",
    "\n",
    "    print(f\"      Jaccard: {r['evaluation']['jaccard']:.3f} | \"\n",
    "          f\"Grounding: {r['evaluation']['grounding_rate']:.1%} | \"\n",
    "          f\"Tokens: {r['usage']['input_tokens']:,} in / {r['usage']['output_tokens']:,} out | \"\n",
    "          f\"Time: {r['usage']['latency_s']:.1f}s\")\n",
    "\n",
    "    # Trace info (M1 only)\n",
    "    if r.get(\"trace\", {}).get(\"nodes_visited\"):\n",
    "        print(f\"      Trace: {' -> '.join(r['trace']['nodes_visited'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4d5e6",
   "metadata": {},
   "source": [
    "## 4. Model Diagnostics & Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7f8a9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Diagnostics (claude-sonnet-4)\n",
      "==================================================\n",
      "API calls:       20\n",
      "Success rate:    100%\n",
      "Input tokens:    169,326\n",
      "Output tokens:   9,475\n",
      "Total tokens:    178,801\n",
      "Estimated cost:  $0.6501\n",
      "Avg latency:     10041 ms\n",
      "Total time:      301.1 s\n",
      "\n",
      "Avg tokens/call: 8,466 in / 474 out\n",
      "\n",
      "Calls by agent:\n",
      "  ip_commercial            : 5\n",
      "  risk_liability           : 5\n",
      "  temporal_renewal         : 10\n"
     ]
    }
   ],
   "source": [
    "diag_summary = diagnostics.summary()\n",
    "\n",
    "print(f\"Model Diagnostics ({MODEL_KEY})\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"API calls:       {diag_summary['total_calls']}\")\n",
    "print(f\"Success rate:    {diag_summary['success_rate']:.0%}\")\n",
    "print(f\"Input tokens:    {diag_summary['total_input_tokens']:,}\")\n",
    "print(f\"Output tokens:   {diag_summary['total_output_tokens']:,}\")\n",
    "print(f\"Total tokens:    {diag_summary['total_tokens']:,}\")\n",
    "print(f\"Estimated cost:  ${diag_summary['total_cost_usd']:.4f}\")\n",
    "print(f\"Avg latency:     {diag_summary['avg_latency_ms']:.0f} ms\")\n",
    "print(f\"Total time:      {diag_summary['duration_seconds']:.1f} s\")\n",
    "\n",
    "if diag_summary[\"total_calls\"] > 0:\n",
    "    avg_in = diag_summary[\"total_input_tokens\"] / diag_summary[\"total_calls\"]\n",
    "    avg_out = diag_summary[\"total_output_tokens\"] / diag_summary[\"total_calls\"]\n",
    "    print(f\"\\nAvg tokens/call: {avg_in:,.0f} in / {avg_out:,.0f} out\")\n",
    "\n",
    "# Per-agent breakdown (useful for M1 to see specialist distribution)\n",
    "if diag_summary.get(\"by_agent\"):\n",
    "    print(f\"\\nCalls by agent:\")\n",
    "    for agent, count in sorted(diag_summary[\"by_agent\"].items()):\n",
    "        print(f\"  {agent:25s}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2b3c5",
   "metadata": {},
   "source": [
    "## 5. Statistical Comparison Against Baselines\n",
    "\n",
    "Load baseline results (B1/B4 from notebook 03) and run hypothesis tests:\n",
    "\n",
    "- **H1**: McNemar + Cohen's d → M-variant vs B1 (F2 improvement)\n",
    "- **H2**: Per-tier F2 comparison → rare vs common improvement\n",
    "- **H3**: McNemar → M1 vs M6 (architecture vs prompts) — requires both M1 and M6 runs\n",
    "- **H4**: Trace completeness from M1 records (check trace nodes present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5e6f7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded baseline results:\n",
      "  B1 (zero_shot)      : 45 samples | F1=0.847 F2=0.839 J=0.305\n",
      "  B4 (cot)            : not found\n",
      "  M6 (combined)       : not found\n",
      "  M1 (multiagent)     : not found\n"
     ]
    }
   ],
   "source": [
    "# ── Load baseline results ──\n",
    "baseline_dir = Path(BASELINE_RESULTS_DIR)\n",
    "\n",
    "def load_latest_run(label_prefix: str) -> tuple[dict | None, list[dict]]:\n",
    "    \"\"\"Find the most recent summary + intermediate files for a given baseline label.\"\"\"\n",
    "    summaries = sorted(baseline_dir.glob(f\"{label_prefix}_*_summary.json\"), reverse=True)\n",
    "    if not summaries:\n",
    "        return None, []\n",
    "    summary_path = summaries[0]\n",
    "    with open(summary_path) as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    # Load per-sample intermediate\n",
    "    inter_path = summary_path.with_name(summary_path.name.replace(\"_summary.json\", \"_intermediate.jsonl\"))\n",
    "    records = []\n",
    "    if inter_path.exists():\n",
    "        with open(inter_path) as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    records.append(json.loads(line))\n",
    "    return summary, records\n",
    "\n",
    "b1_summary, b1_records = load_latest_run(\"zero_shot\")\n",
    "b4_summary, b4_records = load_latest_run(\"cot\")\n",
    "m6_summary, m6_records = load_latest_run(\"combined_prompts\")\n",
    "\n",
    "# Also try loading M1 results if we're running M6 (for H3 comparison)\n",
    "m1_summary, m1_records = load_latest_run(\"multiagent\")\n",
    "\n",
    "print(\"Loaded baseline results:\")\n",
    "for label, summ, recs in [\n",
    "    (\"B1 (zero_shot)\", b1_summary, b1_records),\n",
    "    (\"B4 (cot)\", b4_summary, b4_records),\n",
    "    (\"M6 (combined)\", m6_summary, m6_records),\n",
    "    (\"M1 (multiagent)\", m1_summary, m1_records),\n",
    "]:\n",
    "    if summ:\n",
    "        m = summ[\"metrics\"]\n",
    "        print(f\"  {label:20s}: {len(recs)} samples | F1={m['f1']:.3f} F2={m['f2']:.3f} J={m['avg_jaccard']:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {label:20s}: not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9c0d1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  HYPOTHESIS TESTS\n",
      "======================================================================\n",
      "\n",
      "--- H1: M1 vs B1 (zero-shot baseline) ---\n",
      "  Paired samples: 5\n",
      "Performance: 80.0% Accuracy (95% CI: 40.0%-100.0%)\n",
      "Comparison: +0.0% vs. baseline (p = 0.480, Cohen's d = 0.60)\n",
      "  Jaccard Wilcoxon: W=0.0, p=1.0000\n",
      "\n",
      "--- H1b: M1 vs B4 (chain-of-thought) ---\n",
      "  SKIPPED: B4 results not found\n",
      "\n",
      "--- H2: Per-tier improvement (M1 vs B1) ---\n",
      "  common    : M1=80.0%  B1=80.0%  Δ=+0.0%  (n=5)\n",
      "  moderate: no paired samples\n",
      "  rare: no paired samples\n",
      "\n",
      "--- H3: M1 vs M6 (architecture vs prompts) ---\n",
      "  SKIPPED: Need both M1 and M6 results (missing: M6)\n",
      "\n",
      "--- H4: Trace completeness ---\n",
      "  Records with trace: 20 / 21\n",
      "  Trace completeness: 95.2% (target: > 90%)\n",
      "  PASS: Meets 90% threshold\n",
      "\n",
      "--- Benjamini-Hochberg Correction ---\n",
      "  Test 1: p=0.4795 ns\n",
      "  Test 2: p=1.0000 ns\n",
      "  Significant after BH correction: 0 / 2\n"
     ]
    }
   ],
   "source": [
    "from src.evaluation.statistical import (\n",
    "    bootstrap_ci, mcnemar_test, wilcoxon_test,\n",
    "    benjamini_hochberg, cohens_d, format_result,\n",
    ")\n",
    "\n",
    "# ── Helper: build paired outcome vectors from two sets of records ──\n",
    "def build_paired_outcomes(records_a: list[dict], records_b: list[dict]):\n",
    "    \"\"\"Align two sets of records by sample_id and return paired binary outcomes.\"\"\"\n",
    "    by_id_a = {r[\"sample_id\"]: r for r in records_a}\n",
    "    by_id_b = {r[\"sample_id\"]: r for r in records_b}\n",
    "    shared_ids = sorted(set(by_id_a) & set(by_id_b))\n",
    "\n",
    "    correct_a = [by_id_a[sid][\"evaluation\"][\"classification\"] in (\"TP\", \"TN\") for sid in shared_ids]\n",
    "    correct_b = [by_id_b[sid][\"evaluation\"][\"classification\"] in (\"TP\", \"TN\") for sid in shared_ids]\n",
    "    jaccard_a = [by_id_a[sid][\"evaluation\"][\"jaccard\"] for sid in shared_ids]\n",
    "    jaccard_b = [by_id_b[sid][\"evaluation\"][\"jaccard\"] for sid in shared_ids]\n",
    "    return correct_a, correct_b, jaccard_a, jaccard_b, shared_ids\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  HYPOTHESIS TESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "p_values = []  # Collect for BH correction\n",
    "\n",
    "# ── H1: Multi-agent vs B1 baseline ──\n",
    "print(f\"\\n--- H1: {EXPERIMENT_TYPE} vs B1 (zero-shot baseline) ---\")\n",
    "if b1_records and results:\n",
    "    correct_exp, correct_b1, jacc_exp, jacc_b1, shared = build_paired_outcomes(results, b1_records)\n",
    "    print(f\"  Paired samples: {len(shared)}\")\n",
    "\n",
    "    # McNemar test on binary correctness\n",
    "    chi2, p_val = mcnemar_test(correct_exp, correct_b1)\n",
    "    d = cohens_d(jacc_exp, jacc_b1)\n",
    "    p_values.append(p_val)\n",
    "\n",
    "    exp_acc = sum(correct_exp) / len(correct_exp)\n",
    "    b1_acc = sum(correct_b1) / len(correct_b1)\n",
    "    ci = bootstrap_ci([1 if c else 0 for c in correct_exp])\n",
    "\n",
    "    print(format_result(\"Accuracy\", exp_acc, ci=ci, baseline_value=b1_acc, p_value=p_val, effect_size=d))\n",
    "\n",
    "    # Wilcoxon on Jaccard\n",
    "    w_stat, w_p = wilcoxon_test(jacc_exp, jacc_b1)\n",
    "    p_values.append(w_p)\n",
    "    print(f\"  Jaccard Wilcoxon: W={w_stat:.1f}, p={w_p:.4f}\")\n",
    "else:\n",
    "    print(\"  SKIPPED: B1 results not found\")\n",
    "\n",
    "# ── H1b: Multi-agent vs B4 baseline ──\n",
    "print(f\"\\n--- H1b: {EXPERIMENT_TYPE} vs B4 (chain-of-thought) ---\")\n",
    "if b4_records and results:\n",
    "    correct_exp, correct_b4, jacc_exp, jacc_b4, shared = build_paired_outcomes(results, b4_records)\n",
    "    print(f\"  Paired samples: {len(shared)}\")\n",
    "\n",
    "    chi2, p_val = mcnemar_test(correct_exp, correct_b4)\n",
    "    d = cohens_d(jacc_exp, jacc_b4)\n",
    "    p_values.append(p_val)\n",
    "\n",
    "    exp_acc = sum(correct_exp) / len(correct_exp)\n",
    "    b4_acc = sum(correct_b4) / len(correct_b4)\n",
    "    ci = bootstrap_ci([1 if c else 0 for c in correct_exp])\n",
    "\n",
    "    print(format_result(\"Accuracy\", exp_acc, ci=ci, baseline_value=b4_acc, p_value=p_val, effect_size=d))\n",
    "else:\n",
    "    print(\"  SKIPPED: B4 results not found\")\n",
    "\n",
    "# ── H2: Per-tier improvement (rare vs common) ──\n",
    "print(f\"\\n--- H2: Per-tier improvement ({EXPERIMENT_TYPE} vs B1) ---\")\n",
    "if b1_records and results:\n",
    "    by_id_exp = {r[\"sample_id\"]: r for r in results}\n",
    "    by_id_b1 = {r[\"sample_id\"]: r for r in b1_records}\n",
    "    shared_ids = set(by_id_exp) & set(by_id_b1)\n",
    "\n",
    "    for tier in [\"common\", \"moderate\", \"rare\"]:\n",
    "        tier_ids = [sid for sid in shared_ids if by_id_exp[sid][\"tier\"] == tier]\n",
    "        if not tier_ids:\n",
    "            print(f\"  {tier}: no paired samples\")\n",
    "            continue\n",
    "        tier_correct_exp = [by_id_exp[sid][\"evaluation\"][\"classification\"] in (\"TP\", \"TN\") for sid in tier_ids]\n",
    "        tier_correct_b1 = [by_id_b1[sid][\"evaluation\"][\"classification\"] in (\"TP\", \"TN\") for sid in tier_ids]\n",
    "        exp_rate = sum(tier_correct_exp) / len(tier_correct_exp)\n",
    "        b1_rate = sum(tier_correct_b1) / len(tier_correct_b1)\n",
    "        delta = exp_rate - b1_rate\n",
    "        print(f\"  {tier:10s}: {EXPERIMENT_TYPE}={exp_rate:.1%}  B1={b1_rate:.1%}  Δ={delta:+.1%}  (n={len(tier_ids)})\")\n",
    "else:\n",
    "    print(\"  SKIPPED: B1 results not found\")\n",
    "\n",
    "# ── H3: M1 vs M6 (architecture vs prompts) ──\n",
    "print(f\"\\n--- H3: M1 vs M6 (architecture vs prompts) ---\")\n",
    "# Use stored M1/M6 results if available\n",
    "h3_m1_records = m1_records if EXPERIMENT_TYPE != \"M1\" else results\n",
    "h3_m6_records = m6_records if EXPERIMENT_TYPE != \"M6\" else results\n",
    "\n",
    "if h3_m1_records and h3_m6_records:\n",
    "    correct_m1, correct_m6, jacc_m1, jacc_m6, shared = build_paired_outcomes(h3_m1_records, h3_m6_records)\n",
    "    print(f\"  Paired samples: {len(shared)}\")\n",
    "\n",
    "    chi2, p_val = mcnemar_test(correct_m1, correct_m6)\n",
    "    d = cohens_d(jacc_m1, jacc_m6)\n",
    "    p_values.append(p_val)\n",
    "\n",
    "    m1_acc = sum(correct_m1) / len(correct_m1)\n",
    "    m6_acc = sum(correct_m6) / len(correct_m6)\n",
    "    print(format_result(\"M1 Accuracy\", m1_acc, baseline_value=m6_acc, p_value=p_val, effect_size=d))\n",
    "\n",
    "    if p_val < 0.05 and m1_acc > m6_acc:\n",
    "        print(\"  => Architecture provides genuine benefit beyond prompting\")\n",
    "    elif p_val >= 0.05:\n",
    "        print(\"  => No significant difference: multi-agent overhead may not be justified\")\n",
    "    else:\n",
    "        print(\"  => M6 outperforms M1: combined prompts sufficient\")\n",
    "else:\n",
    "    missing = []\n",
    "    if not h3_m1_records:\n",
    "        missing.append(\"M1\")\n",
    "    if not h3_m6_records:\n",
    "        missing.append(\"M6\")\n",
    "    print(f\"  SKIPPED: Need both M1 and M6 results (missing: {', '.join(missing)})\")\n",
    "\n",
    "# ── H4: Trace completeness (M1 only) ──\n",
    "print(f\"\\n--- H4: Trace completeness ---\")\n",
    "# Check records that have trace info (M1 records)\n",
    "trace_records = h3_m1_records or []\n",
    "if trace_records:\n",
    "    with_trace = [r for r in trace_records if r.get(\"trace\", {}).get(\"nodes_visited\")]\n",
    "    completeness = len(with_trace) / len(trace_records) if trace_records else 0\n",
    "    print(f\"  Records with trace: {len(with_trace)} / {len(trace_records)}\")\n",
    "    print(f\"  Trace completeness: {completeness:.1%} (target: > 90%)\")\n",
    "    print(f\"  {'PASS' if completeness > 0.9 else 'FAIL'}: {'Meets' if completeness > 0.9 else 'Below'} 90% threshold\")\n",
    "else:\n",
    "    print(\"  SKIPPED: No M1 records with trace data available\")\n",
    "\n",
    "# ── Multiple comparison correction ──\n",
    "if p_values:\n",
    "    print(f\"\\n--- Benjamini-Hochberg Correction ---\")\n",
    "    significant = benjamini_hochberg(p_values, alpha=0.05)\n",
    "    for i, (p, sig) in enumerate(zip(p_values, significant)):\n",
    "        print(f\"  Test {i+1}: p={p:.4f} {'*' if sig else 'ns'}\")\n",
    "    print(f\"  Significant after BH correction: {sum(significant)} / {len(significant)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4",
   "metadata": {},
   "source": [
    "## 6. Summary Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "  Cross-Configuration Comparison\n",
      "=====================================================================================\n",
      "  Config      N    Prec     Rec      F1      F2  Jaccard    Lazy\n",
      "  ---------------------------------------------------------------------------\n",
      "  B1         45   0.862   0.833   0.847   0.839    0.305  10.0%\n",
      "  M1         21   0.769   0.667   0.714   0.685    0.334  13.3% <--\n"
     ]
    }
   ],
   "source": [
    "# Build summary table across all available configs\n",
    "configs = []\n",
    "\n",
    "# Current run\n",
    "configs.append({\n",
    "    \"config\": EXPERIMENT_TYPE,\n",
    "    \"f1\": f1, \"f2\": f2, \"precision\": precision, \"recall\": recall,\n",
    "    \"jaccard\": avg_jaccard, \"laziness\": laziness_rate,\n",
    "    \"samples\": len(results),\n",
    "})\n",
    "\n",
    "# Loaded baselines\n",
    "for label, summ in [(\"B1\", b1_summary), (\"B4\", b4_summary), (\"M6\", m6_summary), (\"M1\", m1_summary)]:\n",
    "    if summ and label != EXPERIMENT_TYPE:  # Don't duplicate current run\n",
    "        m = summ[\"metrics\"]\n",
    "        configs.append({\n",
    "            \"config\": label,\n",
    "            \"f1\": m[\"f1\"], \"f2\": m[\"f2\"],\n",
    "            \"precision\": m[\"precision\"], \"recall\": m[\"recall\"],\n",
    "            \"jaccard\": m[\"avg_jaccard\"], \"laziness\": m[\"laziness_rate\"],\n",
    "            \"samples\": sum(m[k] for k in [\"tp\", \"fp\", \"fn\", \"tn\"]),\n",
    "        })\n",
    "\n",
    "# Sort: baselines first, then M-variants\n",
    "order = {\"B1\": 0, \"B4\": 1, \"M6\": 2, \"M1\": 3, \"M2\": 4, \"M3\": 5, \"M4\": 6, \"M5\": 7}\n",
    "configs.sort(key=lambda c: order.get(c[\"config\"], 99))\n",
    "\n",
    "print(f\"{'='*85}\")\n",
    "print(f\"  Cross-Configuration Comparison\")\n",
    "print(f\"{'='*85}\")\n",
    "print(f\"  {'Config':<8} {'N':>4} {'Prec':>7} {'Rec':>7} {'F1':>7} {'F2':>7} {'Jaccard':>8} {'Lazy':>7}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "for c in configs:\n",
    "    marker = \" <--\" if c[\"config\"] == EXPERIMENT_TYPE else \"\"\n",
    "    print(f\"  {c['config']:<8} {c['samples']:>4} {c['precision']:>7.3f} {c['recall']:>7.3f} \"\n",
    "          f\"{c['f1']:>7.3f} {c['f2']:>7.3f} {c['jaccard']:>8.3f} {c['laziness']:>6.1%}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b9",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7a8b9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved:      ../experiments/results/multiagent_claude-sonnet-4_20260216_005035_summary.json\n",
      "Diagnostics saved:  ../experiments/diagnostics/multiagent_claude-sonnet-4_20260216_005035_diagnostics.json\n",
      "Intermediate saved: ../experiments/results/multiagent_claude-sonnet-4_20260216_005035_intermediate.jsonl\n",
      "\n",
      "To inspect a single record:\n",
      "  head -1 ../experiments/results/multiagent_claude-sonnet-4_20260216_005035_intermediate.jsonl | python -m json.tool\n"
     ]
    }
   ],
   "source": [
    "from src.agents.orchestrator import CATEGORY_ROUTING\n",
    "\n",
    "# ── Build architecture description ──\n",
    "architecture = None\n",
    "\n",
    "if EXPERIMENT_TYPE == \"M1\":\n",
    "    # Collect specialist prompt info\n",
    "    specialist_prompts = {}\n",
    "    for name, agent in _specialists.items():\n",
    "        pt = agent.prompt_template\n",
    "        specialist_prompts[name] = {\n",
    "            \"description\": pt.description,\n",
    "            \"version\": pt.version,\n",
    "            \"system_prompt\": pt.system.strip(),\n",
    "            \"categories\": agent.config.categories,\n",
    "            \"category_count\": len(agent.config.categories),\n",
    "        }\n",
    "    architecture = {\n",
    "        \"type\": \"multi_agent\",\n",
    "        \"description\": \"LangGraph orchestrator routes to specialist by category, then validation\",\n",
    "        \"workflow\": [\"route\", \"specialist\", \"validate\", \"finalize\"],\n",
    "        \"specialists\": list(_specialists.keys()),\n",
    "        \"validation_enabled\": _orchestrator.validation_agent is not None,\n",
    "        \"routing_table\": CATEGORY_ROUTING,\n",
    "        \"specialist_prompts\": specialist_prompts,\n",
    "    }\n",
    "\n",
    "elif EXPERIMENT_TYPE == \"M6\":\n",
    "    from src.baselines.combined_prompts import COMBINED_PROMPT\n",
    "    architecture = {\n",
    "        \"type\": \"single_agent_ablation\",\n",
    "        \"description\": \"Single agent with combined specialist prompts (architecture ablation)\",\n",
    "        \"workflow\": [\"combined_prompt\"],\n",
    "        \"system_prompt\": COMBINED_PROMPT.split(\"{contract_text}\")[0].strip(),  # Template portion only\n",
    "    }\n",
    "\n",
    "# ── Build summary ──\n",
    "summary = {\n",
    "    \"run_id\": run_id,\n",
    "    \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "    \"config\": {\n",
    "        \"model_key\": MODEL_KEY,\n",
    "        \"model_id\": config.model_id,\n",
    "        \"provider\": config.provider.value,\n",
    "        \"experiment_type\": EXPERIMENT_TYPE,\n",
    "        \"experiment_label\": experiment_label,\n",
    "        \"samples_per_tier\": SAMPLES_PER_TIER,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"max_contract_chars\": MAX_CONTRACT_CHARS,\n",
    "        \"include_negative\": INCLUDE_NEGATIVE_SAMPLES,\n",
    "    },\n",
    "    \"architecture\": architecture,\n",
    "    \"metrics\": {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2,\n",
    "        \"avg_jaccard\": avg_jaccard,\n",
    "        \"laziness_rate\": laziness_rate,\n",
    "        \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn,\n",
    "    },\n",
    "    \"per_tier\": {},\n",
    "    \"samples\": [],\n",
    "    \"diagnostics\": diag_summary,\n",
    "    \"intermediate_file\": str(intermediate_path),\n",
    "}\n",
    "\n",
    "# Per-tier metrics\n",
    "for tier in [\"common\", \"moderate\", \"rare\"]:\n",
    "    tr = [r for r in results if r[\"tier\"] == tier]\n",
    "    t_tp = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"TP\")\n",
    "    t_fp = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"FP\")\n",
    "    t_fn = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"FN\")\n",
    "    t_tn = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"TN\")\n",
    "    t_jaccs = [r[\"evaluation\"][\"jaccard\"] for r in tr if r[\"ground_truth\"][\"has_clause\"]]\n",
    "    summary[\"per_tier\"][tier] = {\n",
    "        \"tp\": t_tp, \"fp\": t_fp, \"fn\": t_fn, \"tn\": t_tn,\n",
    "        \"f1\": compute_f1(t_tp, t_fp, t_fn),\n",
    "        \"f2\": compute_f2(t_tp, t_fp, t_fn),\n",
    "        \"avg_jaccard\": sum(t_jaccs) / len(t_jaccs) if t_jaccs else 0,\n",
    "    }\n",
    "\n",
    "# Compact per-sample view\n",
    "for r in results:\n",
    "    summary[\"samples\"].append({\n",
    "        \"id\": r[\"sample_id\"],\n",
    "        \"category\": r[\"category\"],\n",
    "        \"tier\": r[\"tier\"],\n",
    "        \"classification\": r[\"evaluation\"][\"classification\"],\n",
    "        \"jaccard\": r[\"evaluation\"][\"jaccard\"],\n",
    "        \"grounding_rate\": r[\"evaluation\"][\"grounding_rate\"],\n",
    "        \"num_clauses_predicted\": r[\"output\"][\"num_clauses\"],\n",
    "        \"num_gt_spans\": r[\"ground_truth\"][\"num_spans\"],\n",
    "        \"input_tokens\": r[\"usage\"][\"input_tokens\"],\n",
    "        \"output_tokens\": r[\"usage\"][\"output_tokens\"],\n",
    "        \"latency_s\": r[\"usage\"][\"latency_s\"],\n",
    "    })\n",
    "\n",
    "# Save summary\n",
    "summary_path = output_dir / f\"{run_id}_summary.json\"\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "print(f\"Summary saved:      {summary_path}\")\n",
    "\n",
    "# Save diagnostics\n",
    "diag_dir = Path(\"../experiments/diagnostics\")\n",
    "diag_dir.mkdir(parents=True, exist_ok=True)\n",
    "diag_path = diag_dir / f\"{run_id}_diagnostics.json\"\n",
    "diagnostics.export(diag_path)\n",
    "print(f\"Diagnostics saved:  {diag_path}\")\n",
    "\n",
    "# Remind about intermediate\n",
    "print(f\"Intermediate saved: {intermediate_path}\")\n",
    "print(f\"\\nTo inspect a single record:\")\n",
    "print(f\"  head -1 {intermediate_path} | python -m json.tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d3",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Switch experiment** — change `EXPERIMENT_TYPE`:\n",
    "```python\n",
    "EXPERIMENT_TYPE = \"M1\"  # Full multi-agent (orchestrator + 3 specialists)\n",
    "EXPERIMENT_TYPE = \"M6\"  # Combined prompts (architecture ablation)\n",
    "# M2–M5 reserved for future ablations\n",
    "```\n",
    "\n",
    "**Workflow for complete comparison:**\n",
    "1. Run notebook 03 with `B1` and `B4` to establish baselines\n",
    "2. Run this notebook with `M1` (core thesis contribution)\n",
    "3. Run this notebook with `M6` (critical ablation)\n",
    "4. Re-run the statistical comparison cell — it auto-loads all available results\n",
    "\n",
    "**Output files:**\n",
    "- `experiments/results/{run_id}_intermediate.jsonl` — full per-sample records\n",
    "- `experiments/results/{run_id}_summary.json` — config + metrics + compact results\n",
    "- `experiments/diagnostics/{run_id}_diagnostics.json` — raw API call log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
