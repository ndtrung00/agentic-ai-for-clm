{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Multi-Agent Experiment (M1–M6)\n",
    "\n",
    "This notebook runs **multi-agent and ablation configurations** on the same stratified\n",
    "CUAD sample used in `03_baseline_calibration.ipynb`, then performs statistical\n",
    "comparison against baselines (B1/B4).\n",
    "\n",
    "**Configurations:**\n",
    "- `M1`: Full multi-agent system (orchestrator + 3 specialists + validation via LangGraph)\n",
    "- `M2`–`M5`: Reserved for ablation studies (not yet implemented)\n",
    "- `M6`: Combined specialist prompts in a single agent (critical ablation: architecture vs prompting)\n",
    "\n",
    "**Key hypotheses tested here:**\n",
    "\n",
    "| ID | Hypothesis | Test |\n",
    "|----|-----------|------|\n",
    "| H1 | Multi-agent beats single-agent baselines | F2(M1) > F2(B1), McNemar p < 0.05 |\n",
    "| H2 | Specialists help rare categories most | ΔF2_rare > ΔF2_common |\n",
    "| H3 | Architecture matters, not just prompts | M1 > M6 significantly |\n",
    "| H4 | Multi-agent produces auditable reasoning | Trace completeness > 90% |\n",
    "\n",
    "**Pipeline:** Same as notebook 03 (crash-safe JSONL, resume support, full traceability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "MODEL_KEY = \"claude-sonnet-4\"       # Model key (see src/models/config.py)\n",
    "EXPERIMENT_TYPE = \"M1\"              # M1=full-multiagent, M6=combined-prompts, M2-M5=reserved\n",
    "SAMPLES_PER_TIER = 5               # Must match baseline runs for fair comparison\n",
    "INCLUDE_NEGATIVE_SAMPLES = True\n",
    "MAX_CONTRACT_CHARS = 100_000\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS = 4096\n",
    "\n",
    "# Path to baseline results for statistical comparison\n",
    "BASELINE_RESULTS_DIR = \"../experiments/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time, json, datetime\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "from src.models.config import get_model_config, ModelProvider\n",
    "\n",
    "config = get_model_config(MODEL_KEY)\n",
    "\n",
    "experiment_labels = {\n",
    "    \"M1\": \"multiagent\",\n",
    "    \"M2\": \"ablation_no_validation\",\n",
    "    \"M3\": \"ablation_single_specialist\",\n",
    "    \"M4\": \"ablation_no_routing\",\n",
    "    \"M5\": \"ablation_no_specialist_prompts\",\n",
    "    \"M6\": \"combined_prompts\",\n",
    "}\n",
    "assert EXPERIMENT_TYPE in experiment_labels, (\n",
    "    f\"Unknown EXPERIMENT_TYPE={EXPERIMENT_TYPE!r}. \"\n",
    "    f\"Valid options: {list(experiment_labels)}\"\n",
    ")\n",
    "experiment_label = experiment_labels[EXPERIMENT_TYPE]\n",
    "\n",
    "print(f\"Model:      {config.name} ({config.model_id})\")\n",
    "print(f\"Provider:   {config.provider.value}\")\n",
    "print(f\"Experiment: {EXPERIMENT_TYPE} ({experiment_label})\")\n",
    "print(f\"Context:    {config.context_window:,} tokens\")\n",
    "\n",
    "# Verify provider connectivity\n",
    "if config.provider == ModelProvider.OLLAMA:\n",
    "    import urllib.request\n",
    "    try:\n",
    "        urllib.request.urlopen(f\"{config.base_url or 'http://localhost:11434/v1'}/models\")\n",
    "        print(\"Ollama:     connected\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING:    Ollama not reachable — {e}\")\n",
    "elif config.provider == ModelProvider.ANTHROPIC:\n",
    "    assert os.getenv(\"ANTHROPIC_API_KEY\"), \"ANTHROPIC_API_KEY not set\"\n",
    "    print(\"API key:    set\")\n",
    "elif config.provider == ModelProvider.OPENAI:\n",
    "    assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not set\"\n",
    "    print(\"API key:    set\")\n",
    "elif config.provider == ModelProvider.GOOGLE:\n",
    "    assert os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\"), \"GEMINI_API_KEY not set\"\n",
    "    print(\"API key:    set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4f5a6",
   "metadata": {},
   "source": [
    "## 1. Load and Sample CUAD Data\n",
    "\n",
    "Identical stratified sampling as notebook 03 (`random.seed(42)`, same tier counts)\n",
    "to ensure fair comparison against baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.cuad_loader import CUADDataLoader, CATEGORY_TIERS\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "loader = CUADDataLoader()\n",
    "loader.load()\n",
    "all_samples = list(loader)\n",
    "\n",
    "print(f\"Total samples: {len(all_samples):,}\")\n",
    "print(f\"Contracts:     {len(loader.get_contracts())}\")\n",
    "print()\n",
    "\n",
    "by_tier: dict[str, list] = defaultdict(list)\n",
    "for s in all_samples:\n",
    "    if len(s.contract_text) <= MAX_CONTRACT_CHARS:\n",
    "        by_tier[s.tier].append(s)\n",
    "\n",
    "for tier in [\"common\", \"moderate\", \"rare\"]:\n",
    "    pos = sum(1 for s in by_tier[tier] if s.has_clause)\n",
    "    neg = len(by_tier[tier]) - pos\n",
    "    print(f\"{tier:10s}: {len(by_tier[tier]):,} samples ({pos} pos, {neg} neg)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = []\n",
    "for tier in [\"common\", \"moderate\", \"rare\"]:\n",
    "    tier_samples = by_tier[tier]\n",
    "    positive = [s for s in tier_samples if s.has_clause]\n",
    "    negative = [s for s in tier_samples if not s.has_clause]\n",
    "\n",
    "    n_pos = min(SAMPLES_PER_TIER, len(positive))\n",
    "    selected.extend(random.sample(positive, n_pos))\n",
    "\n",
    "    if INCLUDE_NEGATIVE_SAMPLES and negative:\n",
    "        n_neg = min(max(1, SAMPLES_PER_TIER // 2), len(negative))\n",
    "        selected.extend(random.sample(negative, n_neg))\n",
    "\n",
    "print(f\"Selected {len(selected)} samples:\\n\")\n",
    "for s in selected:\n",
    "    info = f\"{s.num_spans} spans\" if s.has_clause else \"no clause\"\n",
    "    print(f\"  [{s.tier:8s}] {s.category:40s} ({info}) | {len(s.contract_text):,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8",
   "metadata": {},
   "source": [
    "## 2. Run Extraction\n",
    "\n",
    "Extraction path depends on `EXPERIMENT_TYPE`:\n",
    "\n",
    "- **M1**: Orchestrator routes to specialist agent (risk/temporal/IP) via LangGraph,\n",
    "  then validation layer checks grounding. Full trace captured for H4.\n",
    "- **M6**: Single agent with combined specialist prompts (all domain knowledge in one prompt).\n",
    "  Tests whether multi-agent architecture provides benefit beyond prompt engineering.\n",
    "- **M2–M5**: Reserved for ablation studies (raise error if selected).\n",
    "\n",
    "Same crash-safe JSONL + resume logic as notebook 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import invoke_model as model_invoke\n",
    "from src.models.diagnostics import ModelDiagnostics, TokenUsage\n",
    "from src.evaluation.metrics import span_overlap, compute_jaccard, compute_grounding_rate\n",
    "\n",
    "# ── M1: Multi-Agent Setup ──\n",
    "_orchestrator = None\n",
    "_m1_diagnostics = None\n",
    "\n",
    "if EXPERIMENT_TYPE == \"M1\":\n",
    "    from src.agents.base import AgentConfig\n",
    "    from src.agents import (\n",
    "        Orchestrator,\n",
    "        RiskLiabilityAgent,\n",
    "        TemporalRenewalAgent,\n",
    "        IPCommercialAgent,\n",
    "    )\n",
    "    _m1_diagnostics = ModelDiagnostics()\n",
    "    _risk_config = AgentConfig(name=\"risk_liability\", model_key=MODEL_KEY, prompt_name=\"risk_liability\")\n",
    "    _temporal_config = AgentConfig(name=\"temporal_renewal\", model_key=MODEL_KEY, prompt_name=\"temporal_renewal\")\n",
    "    _ip_config = AgentConfig(name=\"ip_commercial\", model_key=MODEL_KEY, prompt_name=\"ip_commercial\")\n",
    "    _specialists = {\n",
    "        \"risk_liability\": RiskLiabilityAgent(config=_risk_config, diagnostics=_m1_diagnostics),\n",
    "        \"temporal_renewal\": TemporalRenewalAgent(config=_temporal_config, diagnostics=_m1_diagnostics),\n",
    "        \"ip_commercial\": IPCommercialAgent(config=_ip_config, diagnostics=_m1_diagnostics),\n",
    "    }\n",
    "    _orchestrator = Orchestrator(\n",
    "        specialists=_specialists,\n",
    "        validation_agent=None,\n",
    "        config=AgentConfig(name=\"orchestrator\", model_key=MODEL_KEY),\n",
    "    )\n",
    "    print(f\"M1 Orchestrator ready with {len(_specialists)} specialists (validation=None)\")\n",
    "\n",
    "# ── M6: Combined Prompts Setup ──\n",
    "_m6_baseline = None\n",
    "\n",
    "if EXPERIMENT_TYPE == \"M6\":\n",
    "    from src.agents.base import AgentConfig\n",
    "    from src.baselines.combined_prompts import COMBINED_PROMPT, CombinedPromptsBaseline\n",
    "    _m6_baseline = CombinedPromptsBaseline(\n",
    "        config=AgentConfig(name=\"combined_prompts\", model_key=MODEL_KEY),\n",
    "        diagnostics=None,  # We create a separate diagnostics below\n",
    "    )\n",
    "    print(f\"M6 Combined Prompts baseline ready\")\n",
    "\n",
    "# ── M2–M5: Not yet implemented ──\n",
    "if EXPERIMENT_TYPE in (\"M2\", \"M3\", \"M4\", \"M5\"):\n",
    "    raise NotImplementedError(\n",
    "        f\"{EXPERIMENT_TYPE} ablation is not yet implemented. \"\n",
    "        f\"Currently available: M1 (full multi-agent), M6 (combined prompts). \"\n",
    "        f\"See CLAUDE.md for planned ablation definitions.\"\n",
    "    )\n",
    "\n",
    "# ── Run ID and file setup ──\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_id = f\"{experiment_label}_{MODEL_KEY}_{timestamp}\"\n",
    "\n",
    "output_dir = Path(\"../experiments/results\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "intermediate_path = output_dir / f\"{run_id}_intermediate.jsonl\"\n",
    "\n",
    "print(f\"Run ID:       {run_id}\")\n",
    "print(f\"Intermediate: {intermediate_path}\")\n",
    "\n",
    "# ── Resume: load existing completed samples ──\n",
    "results = []\n",
    "completed_ids = set()\n",
    "if intermediate_path.exists():\n",
    "    with open(intermediate_path) as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                rec = json.loads(line)\n",
    "                completed_ids.add(rec[\"sample_id\"])\n",
    "                results.append(rec)\n",
    "    print(f\"Resuming:     {len(completed_ids)} samples already completed\")\n",
    "print()\n",
    "\n",
    "# ── Diagnostics tracker ──\n",
    "diagnostics = _m1_diagnostics if EXPERIMENT_TYPE == \"M1\" else ModelDiagnostics(experiment_id=run_id)\n",
    "if _m6_baseline is not None:\n",
    "    _m6_baseline.diagnostics = diagnostics\n",
    "\n",
    "# ── Extraction loop ──\n",
    "total = len(selected)\n",
    "start_time = time.time()\n",
    "\n",
    "for i, sample in enumerate(selected):\n",
    "    if sample.id in completed_ids:\n",
    "        print(f\"[{i+1}/{total}] {sample.category} — SKIPPED (already done)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[{i+1}/{total}] {sample.category} ({sample.tier})...\", end=\" \", flush=True)\n",
    "\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        trace_nodes = []  # For H4 trace completeness\n",
    "\n",
    "        if EXPERIMENT_TYPE == \"M1\":\n",
    "            # ── M1: Use Orchestrator (LangGraph) ──\n",
    "            n_calls_before = len(diagnostics.calls)\n",
    "            result = await _orchestrator.extract(\n",
    "                contract_text=sample.contract_text,\n",
    "                category=sample.category,\n",
    "                question=sample.question,\n",
    "            )\n",
    "            raw_response = result.reasoning\n",
    "            system_prompt = \"M1 multi-agent (orchestrator → specialist → validation)\"\n",
    "            user_message = f\"Category: {sample.category}\\nQuestion: {sample.question}\"\n",
    "\n",
    "            # Aggregate usage from all calls made during this extraction\n",
    "            recent_calls = diagnostics.calls[n_calls_before:]\n",
    "            agg_input = sum(c.usage.input_tokens for c in recent_calls)\n",
    "            agg_output = sum(c.usage.output_tokens for c in recent_calls)\n",
    "            usage = type(\"Usage\", (), {\n",
    "                \"input_tokens\": agg_input,\n",
    "                \"output_tokens\": agg_output,\n",
    "                \"cache_read_tokens\": 0,\n",
    "                \"cache_creation_tokens\": 0,\n",
    "            })()\n",
    "\n",
    "            # Capture trace nodes for H4\n",
    "            trace_nodes = [c.agent_name for c in recent_calls]\n",
    "\n",
    "        elif EXPERIMENT_TYPE == \"M6\":\n",
    "            # ── M6: Combined Prompts (single-agent ablation) ──\n",
    "            from src.baselines.combined_prompts import COMBINED_PROMPT\n",
    "            system_prompt = None\n",
    "            user_message = COMBINED_PROMPT.format(\n",
    "                category=sample.category,\n",
    "                contract_text=sample.contract_text,\n",
    "                question=sample.question,\n",
    "            )\n",
    "            messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "\n",
    "            raw_response, usage = await model_invoke(\n",
    "                model_key=MODEL_KEY,\n",
    "                messages=messages,\n",
    "                system=system_prompt,\n",
    "                temperature=TEMPERATURE,\n",
    "                max_tokens=MAX_TOKENS,\n",
    "                diagnostics=diagnostics,\n",
    "                agent_name=\"combined_prompts\",\n",
    "                category=sample.category,\n",
    "            )\n",
    "\n",
    "            # Parse M6 response (JSON or plaintext fallback)\n",
    "            data = _m6_baseline.parse_json_response(raw_response)\n",
    "            if data and \"extracted_clauses\" in data:\n",
    "                result = _m6_baseline.result_from_dict(data, sample.category)\n",
    "            else:\n",
    "                result = _m6_baseline._parse_plaintext(raw_response, sample.category)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        # 4. Evaluate\n",
    "        predicted_text = \" \".join(result.extracted_clauses)\n",
    "        has_prediction = len(result.extracted_clauses) > 0\n",
    "\n",
    "        if sample.has_clause:\n",
    "            if has_prediction:\n",
    "                covers = any(\n",
    "                    span_overlap(predicted_text, gt)\n",
    "                    for gt in sample.ground_truth_spans\n",
    "                )\n",
    "                classification = \"TP\" if covers else \"FN\"\n",
    "            else:\n",
    "                classification = \"FN\"\n",
    "        else:\n",
    "            classification = \"FP\" if has_prediction else \"TN\"\n",
    "\n",
    "        jacc = (\n",
    "            compute_jaccard(predicted_text, sample.ground_truth)\n",
    "            if sample.has_clause and has_prediction\n",
    "            else (1.0 if not sample.has_clause and not has_prediction else 0.0)\n",
    "        )\n",
    "        grounding = (\n",
    "            compute_grounding_rate(result.extracted_clauses, sample.contract_text)\n",
    "            if has_prediction else 1.0\n",
    "        )\n",
    "\n",
    "        # 5. Build full traceable record\n",
    "        record = {\n",
    "            \"sample_id\": sample.id,\n",
    "            \"run_id\": run_id,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"model_key\": MODEL_KEY,\n",
    "            \"model_id\": config.model_id,\n",
    "            \"experiment_type\": EXPERIMENT_TYPE,\n",
    "            \"experiment_label\": experiment_label,\n",
    "            \"category\": sample.category,\n",
    "            \"tier\": sample.tier,\n",
    "            \"contract_title\": sample.contract_title,\n",
    "            \"contract_chars\": len(sample.contract_text),\n",
    "            \"input\": {\n",
    "                \"system_prompt\": system_prompt,\n",
    "                \"user_message_length\": len(user_message),\n",
    "                \"question\": sample.question,\n",
    "            },\n",
    "            \"output\": {\n",
    "                \"raw_response\": raw_response,\n",
    "                \"parsed_clauses\": result.extracted_clauses,\n",
    "                \"num_clauses\": len(result.extracted_clauses),\n",
    "                \"reasoning\": result.reasoning,\n",
    "                \"confidence\": result.confidence,\n",
    "            },\n",
    "            \"ground_truth\": {\n",
    "                \"has_clause\": sample.has_clause,\n",
    "                \"spans\": sample.ground_truth_spans,\n",
    "                \"full_text\": sample.ground_truth,\n",
    "                \"num_spans\": sample.num_spans,\n",
    "            },\n",
    "            \"evaluation\": {\n",
    "                \"classification\": classification,\n",
    "                \"jaccard\": jacc,\n",
    "                \"grounding_rate\": grounding,\n",
    "            },\n",
    "            \"usage\": {\n",
    "                \"input_tokens\": getattr(usage, \"input_tokens\", 0),\n",
    "                \"output_tokens\": getattr(usage, \"output_tokens\", 0),\n",
    "                \"cache_read_tokens\": getattr(usage, \"cache_read_tokens\", 0),\n",
    "                \"cache_creation_tokens\": getattr(usage, \"cache_creation_tokens\", 0),\n",
    "                \"latency_s\": round(elapsed, 2),\n",
    "            },\n",
    "            \"trace\": {\n",
    "                \"nodes_visited\": trace_nodes,\n",
    "                \"num_llm_calls\": len(trace_nodes) if trace_nodes else 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # 6. Append to JSONL immediately (crash-safe)\n",
    "        with open(intermediate_path, \"a\") as f:\n",
    "            f.write(json.dumps(record, default=str) + \"\\n\")\n",
    "\n",
    "        results.append(record)\n",
    "        print(f\"-> {classification} | {len(result.extracted_clauses)} clause(s) | J={jacc:.3f} | {elapsed:.1f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"-> ERROR: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nCompleted: {len(results)} total ({len(completed_ids)} resumed)\")\n",
    "print(f\"Intermediate saved to: {intermediate_path}\")\n",
    "print(f\"Total wall time: {total_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics\n",
    "\n",
    "Same ContractEval definitions as notebook 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.metrics import compute_f1, compute_f2, compute_precision, compute_recall\n",
    "\n",
    "tp = sum(1 for r in results if r[\"evaluation\"][\"classification\"] == \"TP\")\n",
    "fp = sum(1 for r in results if r[\"evaluation\"][\"classification\"] == \"FP\")\n",
    "fn = sum(1 for r in results if r[\"evaluation\"][\"classification\"] == \"FN\")\n",
    "tn = sum(1 for r in results if r[\"evaluation\"][\"classification\"] == \"TN\")\n",
    "\n",
    "total_positive = tp + fn\n",
    "laziness_count = sum(\n",
    "    1 for r in results\n",
    "    if r[\"evaluation\"][\"classification\"] == \"FN\"\n",
    "    and r[\"output\"][\"num_clauses\"] == 0\n",
    ")\n",
    "\n",
    "precision = compute_precision(tp, fp)\n",
    "recall = compute_recall(tp, fn)\n",
    "f1 = compute_f1(tp, fp, fn)\n",
    "f2 = compute_f2(tp, fp, fn)\n",
    "\n",
    "jaccard_scores = [r[\"evaluation\"][\"jaccard\"] for r in results if r[\"ground_truth\"][\"has_clause\"]]\n",
    "avg_jaccard = sum(jaccard_scores) / len(jaccard_scores) if jaccard_scores else 0\n",
    "laziness_rate = laziness_count / total_positive if total_positive > 0 else 0\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  {EXPERIMENT_TYPE} {experiment_label} — {MODEL_KEY}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Samples:       {len(results)}\")\n",
    "print(f\"  TP: {tp}  FP: {fp}  FN: {fn}  TN: {tn}\")\n",
    "print()\n",
    "print(f\"  Precision:     {precision:.3f}\")\n",
    "print(f\"  Recall:        {recall:.3f}\")\n",
    "print(f\"  F1:            {f1:.3f}\")\n",
    "print(f\"  F2:            {f2:.3f}\")\n",
    "print(f\"  Avg Jaccard:   {avg_jaccard:.3f}\")\n",
    "print(f\"  Laziness rate: {laziness_rate:.1%} ({laziness_count}/{total_positive})\")\n",
    "print()\n",
    "print(f\"  ContractEval reference (GPT-4.1):\")\n",
    "print(f\"  F1=0.641  F2=0.678  Jaccard=0.472  Laziness=7.1%\")\n",
    "\n",
    "# Per-tier breakdown\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"  Per-Tier Breakdown\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  {'Tier':<10} {'TP':>4} {'FP':>4} {'FN':>4} {'TN':>4} {'F1':>7} {'F2':>7} {'Jaccard':>8}\")\n",
    "print(f\"  {'-'*60}\")\n",
    "\n",
    "for tier in [\"common\", \"moderate\", \"rare\"]:\n",
    "    tr = [r for r in results if r[\"tier\"] == tier]\n",
    "    t_tp = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"TP\")\n",
    "    t_fp = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"FP\")\n",
    "    t_fn = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"FN\")\n",
    "    t_tn = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"TN\")\n",
    "    t_f1 = compute_f1(t_tp, t_fp, t_fn)\n",
    "    t_f2 = compute_f2(t_tp, t_fp, t_fn)\n",
    "    t_jaccs = [r[\"evaluation\"][\"jaccard\"] for r in tr if r[\"ground_truth\"][\"has_clause\"]]\n",
    "    t_jacc = sum(t_jaccs) / len(t_jaccs) if t_jaccs else 0\n",
    "    print(f\"  {tier:<10} {t_tp:>4} {t_fp:>4} {t_fn:>4} {t_tn:>4} {t_f1:>7.3f} {t_f2:>7.3f} {t_jacc:>8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"  Per-Sample Results\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    cls = r[\"evaluation\"][\"classification\"]\n",
    "    ok = cls in (\"TP\", \"TN\")\n",
    "\n",
    "    print(f\"\\n  [{i+1}] {'PASS' if ok else 'FAIL'} {cls} | {r['category']} ({r['tier']})\")\n",
    "    print(f\"      Contract: {r['contract_title'][:60]}\")\n",
    "    print(f\"      Question: {r['input']['question'][:80]}...\")\n",
    "\n",
    "    if r[\"ground_truth\"][\"has_clause\"]:\n",
    "        gt = r[\"ground_truth\"][\"full_text\"][:120]\n",
    "        print(f\"      GT:   {gt}...\")\n",
    "\n",
    "    if r[\"output\"][\"num_clauses\"] > 0:\n",
    "        pred = r[\"output\"][\"parsed_clauses\"][0][:120]\n",
    "        print(f\"      Pred: {pred}...\")\n",
    "    else:\n",
    "        print(f\"      Pred: (no clause extracted)\")\n",
    "\n",
    "    print(f\"      Jaccard: {r['evaluation']['jaccard']:.3f} | \"\n",
    "          f\"Grounding: {r['evaluation']['grounding_rate']:.1%} | \"\n",
    "          f\"Tokens: {r['usage']['input_tokens']:,} in / {r['usage']['output_tokens']:,} out | \"\n",
    "          f\"Time: {r['usage']['latency_s']:.1f}s\")\n",
    "\n",
    "    # Trace info (M1 only)\n",
    "    if r.get(\"trace\", {}).get(\"nodes_visited\"):\n",
    "        print(f\"      Trace: {' -> '.join(r['trace']['nodes_visited'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4d5e6",
   "metadata": {},
   "source": [
    "## 4. Model Diagnostics & Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_summary = diagnostics.summary()\n",
    "\n",
    "print(f\"Model Diagnostics ({MODEL_KEY})\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"API calls:       {diag_summary['total_calls']}\")\n",
    "print(f\"Success rate:    {diag_summary['success_rate']:.0%}\")\n",
    "print(f\"Input tokens:    {diag_summary['total_input_tokens']:,}\")\n",
    "print(f\"Output tokens:   {diag_summary['total_output_tokens']:,}\")\n",
    "print(f\"Total tokens:    {diag_summary['total_tokens']:,}\")\n",
    "print(f\"Estimated cost:  ${diag_summary['total_cost_usd']:.4f}\")\n",
    "print(f\"Avg latency:     {diag_summary['avg_latency_ms']:.0f} ms\")\n",
    "print(f\"Total time:      {diag_summary['duration_seconds']:.1f} s\")\n",
    "\n",
    "if diag_summary[\"total_calls\"] > 0:\n",
    "    avg_in = diag_summary[\"total_input_tokens\"] / diag_summary[\"total_calls\"]\n",
    "    avg_out = diag_summary[\"total_output_tokens\"] / diag_summary[\"total_calls\"]\n",
    "    print(f\"\\nAvg tokens/call: {avg_in:,.0f} in / {avg_out:,.0f} out\")\n",
    "\n",
    "# Per-agent breakdown (useful for M1 to see specialist distribution)\n",
    "if diag_summary.get(\"by_agent\"):\n",
    "    print(f\"\\nCalls by agent:\")\n",
    "    for agent, count in sorted(diag_summary[\"by_agent\"].items()):\n",
    "        print(f\"  {agent:25s}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2b3c5",
   "metadata": {},
   "source": [
    "## 5. Statistical Comparison Against Baselines\n",
    "\n",
    "Load baseline results (B1/B4 from notebook 03) and run hypothesis tests:\n",
    "\n",
    "- **H1**: McNemar + Cohen's d → M-variant vs B1 (F2 improvement)\n",
    "- **H2**: Per-tier F2 comparison → rare vs common improvement\n",
    "- **H3**: McNemar → M1 vs M6 (architecture vs prompts) — requires both M1 and M6 runs\n",
    "- **H4**: Trace completeness from M1 records (check trace nodes present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load baseline results ──\n",
    "baseline_dir = Path(BASELINE_RESULTS_DIR)\n",
    "\n",
    "def load_latest_run(label_prefix: str) -> tuple[dict | None, list[dict]]:\n",
    "    \"\"\"Find the most recent summary + intermediate files for a given baseline label.\"\"\"\n",
    "    summaries = sorted(baseline_dir.glob(f\"{label_prefix}_*_summary.json\"), reverse=True)\n",
    "    if not summaries:\n",
    "        return None, []\n",
    "    summary_path = summaries[0]\n",
    "    with open(summary_path) as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    # Load per-sample intermediate\n",
    "    inter_path = summary_path.with_name(summary_path.name.replace(\"_summary.json\", \"_intermediate.jsonl\"))\n",
    "    records = []\n",
    "    if inter_path.exists():\n",
    "        with open(inter_path) as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    records.append(json.loads(line))\n",
    "    return summary, records\n",
    "\n",
    "b1_summary, b1_records = load_latest_run(\"zero_shot\")\n",
    "b4_summary, b4_records = load_latest_run(\"cot\")\n",
    "m6_summary, m6_records = load_latest_run(\"combined_prompts\")\n",
    "\n",
    "# Also try loading M1 results if we're running M6 (for H3 comparison)\n",
    "m1_summary, m1_records = load_latest_run(\"multiagent\")\n",
    "\n",
    "print(\"Loaded baseline results:\")\n",
    "for label, summ, recs in [\n",
    "    (\"B1 (zero_shot)\", b1_summary, b1_records),\n",
    "    (\"B4 (cot)\", b4_summary, b4_records),\n",
    "    (\"M6 (combined)\", m6_summary, m6_records),\n",
    "    (\"M1 (multiagent)\", m1_summary, m1_records),\n",
    "]:\n",
    "    if summ:\n",
    "        m = summ[\"metrics\"]\n",
    "        print(f\"  {label:20s}: {len(recs)} samples | F1={m['f1']:.3f} F2={m['f2']:.3f} J={m['avg_jaccard']:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {label:20s}: not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.statistical import (\n",
    "    bootstrap_ci, mcnemar_test, wilcoxon_test,\n",
    "    benjamini_hochberg, cohens_d, format_result,\n",
    ")\n",
    "\n",
    "# ── Helper: build paired outcome vectors from two sets of records ──\n",
    "def build_paired_outcomes(records_a: list[dict], records_b: list[dict]):\n",
    "    \"\"\"Align two sets of records by sample_id and return paired binary outcomes.\"\"\"\n",
    "    by_id_a = {r[\"sample_id\"]: r for r in records_a}\n",
    "    by_id_b = {r[\"sample_id\"]: r for r in records_b}\n",
    "    shared_ids = sorted(set(by_id_a) & set(by_id_b))\n",
    "\n",
    "    correct_a = [by_id_a[sid][\"evaluation\"][\"classification\"] in (\"TP\", \"TN\") for sid in shared_ids]\n",
    "    correct_b = [by_id_b[sid][\"evaluation\"][\"classification\"] in (\"TP\", \"TN\") for sid in shared_ids]\n",
    "    jaccard_a = [by_id_a[sid][\"evaluation\"][\"jaccard\"] for sid in shared_ids]\n",
    "    jaccard_b = [by_id_b[sid][\"evaluation\"][\"jaccard\"] for sid in shared_ids]\n",
    "    return correct_a, correct_b, jaccard_a, jaccard_b, shared_ids\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  HYPOTHESIS TESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "p_values = []  # Collect for BH correction\n",
    "\n",
    "# ── H1: Multi-agent vs B1 baseline ──\n",
    "print(f\"\\n--- H1: {EXPERIMENT_TYPE} vs B1 (zero-shot baseline) ---\")\n",
    "if b1_records and results:\n",
    "    correct_exp, correct_b1, jacc_exp, jacc_b1, shared = build_paired_outcomes(results, b1_records)\n",
    "    print(f\"  Paired samples: {len(shared)}\")\n",
    "\n",
    "    # McNemar test on binary correctness\n",
    "    chi2, p_val = mcnemar_test(correct_exp, correct_b1)\n",
    "    d = cohens_d(jacc_exp, jacc_b1)\n",
    "    p_values.append(p_val)\n",
    "\n",
    "    exp_acc = sum(correct_exp) / len(correct_exp)\n",
    "    b1_acc = sum(correct_b1) / len(correct_b1)\n",
    "    ci = bootstrap_ci([1 if c else 0 for c in correct_exp])\n",
    "\n",
    "    print(format_result(\"Accuracy\", exp_acc, ci=ci, baseline_value=b1_acc, p_value=p_val, effect_size=d))\n",
    "\n",
    "    # Wilcoxon on Jaccard\n",
    "    w_stat, w_p = wilcoxon_test(jacc_exp, jacc_b1)\n",
    "    p_values.append(w_p)\n",
    "    print(f\"  Jaccard Wilcoxon: W={w_stat:.1f}, p={w_p:.4f}\")\n",
    "else:\n",
    "    print(\"  SKIPPED: B1 results not found\")\n",
    "\n",
    "# ── H1b: Multi-agent vs B4 baseline ──\n",
    "print(f\"\\n--- H1b: {EXPERIMENT_TYPE} vs B4 (chain-of-thought) ---\")\n",
    "if b4_records and results:\n",
    "    correct_exp, correct_b4, jacc_exp, jacc_b4, shared = build_paired_outcomes(results, b4_records)\n",
    "    print(f\"  Paired samples: {len(shared)}\")\n",
    "\n",
    "    chi2, p_val = mcnemar_test(correct_exp, correct_b4)\n",
    "    d = cohens_d(jacc_exp, jacc_b4)\n",
    "    p_values.append(p_val)\n",
    "\n",
    "    exp_acc = sum(correct_exp) / len(correct_exp)\n",
    "    b4_acc = sum(correct_b4) / len(correct_b4)\n",
    "    ci = bootstrap_ci([1 if c else 0 for c in correct_exp])\n",
    "\n",
    "    print(format_result(\"Accuracy\", exp_acc, ci=ci, baseline_value=b4_acc, p_value=p_val, effect_size=d))\n",
    "else:\n",
    "    print(\"  SKIPPED: B4 results not found\")\n",
    "\n",
    "# ── H2: Per-tier improvement (rare vs common) ──\n",
    "print(f\"\\n--- H2: Per-tier improvement ({EXPERIMENT_TYPE} vs B1) ---\")\n",
    "if b1_records and results:\n",
    "    by_id_exp = {r[\"sample_id\"]: r for r in results}\n",
    "    by_id_b1 = {r[\"sample_id\"]: r for r in b1_records}\n",
    "    shared_ids = set(by_id_exp) & set(by_id_b1)\n",
    "\n",
    "    for tier in [\"common\", \"moderate\", \"rare\"]:\n",
    "        tier_ids = [sid for sid in shared_ids if by_id_exp[sid][\"tier\"] == tier]\n",
    "        if not tier_ids:\n",
    "            print(f\"  {tier}: no paired samples\")\n",
    "            continue\n",
    "        tier_correct_exp = [by_id_exp[sid][\"evaluation\"][\"classification\"] in (\"TP\", \"TN\") for sid in tier_ids]\n",
    "        tier_correct_b1 = [by_id_b1[sid][\"evaluation\"][\"classification\"] in (\"TP\", \"TN\") for sid in tier_ids]\n",
    "        exp_rate = sum(tier_correct_exp) / len(tier_correct_exp)\n",
    "        b1_rate = sum(tier_correct_b1) / len(tier_correct_b1)\n",
    "        delta = exp_rate - b1_rate\n",
    "        print(f\"  {tier:10s}: {EXPERIMENT_TYPE}={exp_rate:.1%}  B1={b1_rate:.1%}  Δ={delta:+.1%}  (n={len(tier_ids)})\")\n",
    "else:\n",
    "    print(\"  SKIPPED: B1 results not found\")\n",
    "\n",
    "# ── H3: M1 vs M6 (architecture vs prompts) ──\n",
    "print(f\"\\n--- H3: M1 vs M6 (architecture vs prompts) ---\")\n",
    "# Use stored M1/M6 results if available\n",
    "h3_m1_records = m1_records if EXPERIMENT_TYPE != \"M1\" else results\n",
    "h3_m6_records = m6_records if EXPERIMENT_TYPE != \"M6\" else results\n",
    "\n",
    "if h3_m1_records and h3_m6_records:\n",
    "    correct_m1, correct_m6, jacc_m1, jacc_m6, shared = build_paired_outcomes(h3_m1_records, h3_m6_records)\n",
    "    print(f\"  Paired samples: {len(shared)}\")\n",
    "\n",
    "    chi2, p_val = mcnemar_test(correct_m1, correct_m6)\n",
    "    d = cohens_d(jacc_m1, jacc_m6)\n",
    "    p_values.append(p_val)\n",
    "\n",
    "    m1_acc = sum(correct_m1) / len(correct_m1)\n",
    "    m6_acc = sum(correct_m6) / len(correct_m6)\n",
    "    print(format_result(\"M1 Accuracy\", m1_acc, baseline_value=m6_acc, p_value=p_val, effect_size=d))\n",
    "\n",
    "    if p_val < 0.05 and m1_acc > m6_acc:\n",
    "        print(\"  => Architecture provides genuine benefit beyond prompting\")\n",
    "    elif p_val >= 0.05:\n",
    "        print(\"  => No significant difference: multi-agent overhead may not be justified\")\n",
    "    else:\n",
    "        print(\"  => M6 outperforms M1: combined prompts sufficient\")\n",
    "else:\n",
    "    missing = []\n",
    "    if not h3_m1_records:\n",
    "        missing.append(\"M1\")\n",
    "    if not h3_m6_records:\n",
    "        missing.append(\"M6\")\n",
    "    print(f\"  SKIPPED: Need both M1 and M6 results (missing: {', '.join(missing)})\")\n",
    "\n",
    "# ── H4: Trace completeness (M1 only) ──\n",
    "print(f\"\\n--- H4: Trace completeness ---\")\n",
    "# Check records that have trace info (M1 records)\n",
    "trace_records = h3_m1_records or []\n",
    "if trace_records:\n",
    "    with_trace = [r for r in trace_records if r.get(\"trace\", {}).get(\"nodes_visited\")]\n",
    "    completeness = len(with_trace) / len(trace_records) if trace_records else 0\n",
    "    print(f\"  Records with trace: {len(with_trace)} / {len(trace_records)}\")\n",
    "    print(f\"  Trace completeness: {completeness:.1%} (target: > 90%)\")\n",
    "    print(f\"  {'PASS' if completeness > 0.9 else 'FAIL'}: {'Meets' if completeness > 0.9 else 'Below'} 90% threshold\")\n",
    "else:\n",
    "    print(\"  SKIPPED: No M1 records with trace data available\")\n",
    "\n",
    "# ── Multiple comparison correction ──\n",
    "if p_values:\n",
    "    print(f\"\\n--- Benjamini-Hochberg Correction ---\")\n",
    "    significant = benjamini_hochberg(p_values, alpha=0.05)\n",
    "    for i, (p, sig) in enumerate(zip(p_values, significant)):\n",
    "        print(f\"  Test {i+1}: p={p:.4f} {'*' if sig else 'ns'}\")\n",
    "    print(f\"  Significant after BH correction: {sum(significant)} / {len(significant)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4",
   "metadata": {},
   "source": [
    "## 6. Summary Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build summary table across all available configs\n",
    "configs = []\n",
    "\n",
    "# Current run\n",
    "configs.append({\n",
    "    \"config\": EXPERIMENT_TYPE,\n",
    "    \"f1\": f1, \"f2\": f2, \"precision\": precision, \"recall\": recall,\n",
    "    \"jaccard\": avg_jaccard, \"laziness\": laziness_rate,\n",
    "    \"samples\": len(results),\n",
    "})\n",
    "\n",
    "# Loaded baselines\n",
    "for label, summ in [(\"B1\", b1_summary), (\"B4\", b4_summary), (\"M6\", m6_summary), (\"M1\", m1_summary)]:\n",
    "    if summ and label != EXPERIMENT_TYPE:  # Don't duplicate current run\n",
    "        m = summ[\"metrics\"]\n",
    "        configs.append({\n",
    "            \"config\": label,\n",
    "            \"f1\": m[\"f1\"], \"f2\": m[\"f2\"],\n",
    "            \"precision\": m[\"precision\"], \"recall\": m[\"recall\"],\n",
    "            \"jaccard\": m[\"avg_jaccard\"], \"laziness\": m[\"laziness_rate\"],\n",
    "            \"samples\": sum(m[k] for k in [\"tp\", \"fp\", \"fn\", \"tn\"]),\n",
    "        })\n",
    "\n",
    "# Sort: baselines first, then M-variants\n",
    "order = {\"B1\": 0, \"B4\": 1, \"M6\": 2, \"M1\": 3, \"M2\": 4, \"M3\": 5, \"M4\": 6, \"M5\": 7}\n",
    "configs.sort(key=lambda c: order.get(c[\"config\"], 99))\n",
    "\n",
    "print(f\"{'='*85}\")\n",
    "print(f\"  Cross-Configuration Comparison\")\n",
    "print(f\"{'='*85}\")\n",
    "print(f\"  {'Config':<8} {'N':>4} {'Prec':>7} {'Rec':>7} {'F1':>7} {'F2':>7} {'Jaccard':>8} {'Lazy':>7}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "for c in configs:\n",
    "    marker = \" <--\" if c[\"config\"] == EXPERIMENT_TYPE else \"\"\n",
    "    print(f\"  {c['config']:<8} {c['samples']:>4} {c['precision']:>7.3f} {c['recall']:>7.3f} \"\n",
    "          f\"{c['f1']:>7.3f} {c['f2']:>7.3f} {c['jaccard']:>8.3f} {c['laziness']:>6.1%}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b9",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"run_id\": run_id,\n",
    "    \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "    \"config\": {\n",
    "        \"model_key\": MODEL_KEY,\n",
    "        \"model_id\": config.model_id,\n",
    "        \"provider\": config.provider.value,\n",
    "        \"experiment_type\": EXPERIMENT_TYPE,\n",
    "        \"experiment_label\": experiment_label,\n",
    "        \"samples_per_tier\": SAMPLES_PER_TIER,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"max_contract_chars\": MAX_CONTRACT_CHARS,\n",
    "        \"include_negative\": INCLUDE_NEGATIVE_SAMPLES,\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2,\n",
    "        \"avg_jaccard\": avg_jaccard,\n",
    "        \"laziness_rate\": laziness_rate,\n",
    "        \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn,\n",
    "    },\n",
    "    \"per_tier\": {},\n",
    "    \"samples\": [],\n",
    "    \"diagnostics\": diag_summary,\n",
    "    \"intermediate_file\": str(intermediate_path),\n",
    "}\n",
    "\n",
    "# Per-tier metrics\n",
    "for tier in [\"common\", \"moderate\", \"rare\"]:\n",
    "    tr = [r for r in results if r[\"tier\"] == tier]\n",
    "    t_tp = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"TP\")\n",
    "    t_fp = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"FP\")\n",
    "    t_fn = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"FN\")\n",
    "    t_tn = sum(1 for r in tr if r[\"evaluation\"][\"classification\"] == \"TN\")\n",
    "    t_jaccs = [r[\"evaluation\"][\"jaccard\"] for r in tr if r[\"ground_truth\"][\"has_clause\"]]\n",
    "    summary[\"per_tier\"][tier] = {\n",
    "        \"tp\": t_tp, \"fp\": t_fp, \"fn\": t_fn, \"tn\": t_tn,\n",
    "        \"f1\": compute_f1(t_tp, t_fp, t_fn),\n",
    "        \"f2\": compute_f2(t_tp, t_fp, t_fn),\n",
    "        \"avg_jaccard\": sum(t_jaccs) / len(t_jaccs) if t_jaccs else 0,\n",
    "    }\n",
    "\n",
    "# Compact per-sample view\n",
    "for r in results:\n",
    "    summary[\"samples\"].append({\n",
    "        \"id\": r[\"sample_id\"],\n",
    "        \"category\": r[\"category\"],\n",
    "        \"tier\": r[\"tier\"],\n",
    "        \"classification\": r[\"evaluation\"][\"classification\"],\n",
    "        \"jaccard\": r[\"evaluation\"][\"jaccard\"],\n",
    "        \"grounding_rate\": r[\"evaluation\"][\"grounding_rate\"],\n",
    "        \"num_clauses_predicted\": r[\"output\"][\"num_clauses\"],\n",
    "        \"num_gt_spans\": r[\"ground_truth\"][\"num_spans\"],\n",
    "        \"input_tokens\": r[\"usage\"][\"input_tokens\"],\n",
    "        \"output_tokens\": r[\"usage\"][\"output_tokens\"],\n",
    "        \"latency_s\": r[\"usage\"][\"latency_s\"],\n",
    "    })\n",
    "\n",
    "# Save summary\n",
    "summary_path = output_dir / f\"{run_id}_summary.json\"\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "print(f\"Summary saved:      {summary_path}\")\n",
    "\n",
    "# Save diagnostics\n",
    "diag_dir = Path(\"../experiments/diagnostics\")\n",
    "diag_dir.mkdir(parents=True, exist_ok=True)\n",
    "diag_path = diag_dir / f\"{run_id}_diagnostics.json\"\n",
    "diagnostics.export(diag_path)\n",
    "print(f\"Diagnostics saved:  {diag_path}\")\n",
    "\n",
    "# Remind about intermediate\n",
    "print(f\"Intermediate saved: {intermediate_path}\")\n",
    "print(f\"\\nTo inspect a single record:\")\n",
    "print(f\"  head -1 {intermediate_path} | python -m json.tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d3",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Switch experiment** — change `EXPERIMENT_TYPE`:\n",
    "```python\n",
    "EXPERIMENT_TYPE = \"M1\"  # Full multi-agent (orchestrator + 3 specialists)\n",
    "EXPERIMENT_TYPE = \"M6\"  # Combined prompts (architecture ablation)\n",
    "# M2–M5 reserved for future ablations\n",
    "```\n",
    "\n",
    "**Workflow for complete comparison:**\n",
    "1. Run notebook 03 with `B1` and `B4` to establish baselines\n",
    "2. Run this notebook with `M1` (core thesis contribution)\n",
    "3. Run this notebook with `M6` (critical ablation)\n",
    "4. Re-run the statistical comparison cell — it auto-loads all available results\n",
    "\n",
    "**Output files:**\n",
    "- `experiments/results/{run_id}_intermediate.jsonl` — full per-sample records\n",
    "- `experiments/results/{run_id}_summary.json` — config + metrics + compact results\n",
    "- `experiments/diagnostics/{run_id}_diagnostics.json` — raw API call log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
